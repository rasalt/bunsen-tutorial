{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering\n",
    "This notebook steps through a Data Engineering exercise that joins several FHIR resources and converts them at scale into a timeseries-like data model. This notebook should be run after the getting_started notebook. \n",
    "\n",
    "First let's import some needed methods and create our Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session that uses local execution, warehouse, and metadata,\n",
    "# with Hive support to save to tables.\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make sure the imported data exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql('use tutorial_small')\n",
    "spark.sql('show tables').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuesets\n",
    "Common code systems or valuesets, such as those [defined by the VSAC](https://vsac.nlm.nih.gov/valuesets) can be used in the cluster. Here we mostly use user-provided constants to keep the example simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunsen.stu3.valuesets import push_valuesets, valueset\n",
    "\n",
    "# Typically these would the isa_loinc or isa_snomed functions, but\n",
    "# we didn't want the tutorial to require downloading those ontologies.\n",
    "push_valuesets(spark, \n",
    "               {'ldl'               : [('http://loinc.org', '18262-6')],                \n",
    "                'hdl'               : [('http://loinc.org', '2085-9')],\n",
    "                'triglycerides'     : [('http://loinc.org', '2571-8')],\n",
    "                'hba1c'             : [('http://loinc.org', '4548-4')], \n",
    "                'chd'               : [('http://snomed.info/sct', '53741008')],\n",
    "                'hypertension'      : [('http://snomed.info/sct', '38341003')],\n",
    "                'mi'                : [('http://snomed.info/sct', '22298006')], \n",
    "                # The following were loaded in the getting started notebook.\n",
    "                'diabetes_risks'    : valueset('http://engineering.cerner.com/bunsen/example', '201806001'),                \n",
    "                'example'           : valueset('http://hl7.org/fhir/ValueSet/example-extensional', '20150622')},\n",
    "               database='tutorial_ontologies');   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projected onto simple tables\n",
    "As seen in the Getting Started notebook, we can interactively query FHIR datasets and project them onto simple tables. Notice the use of the in_valueset UDF that references the valueset names seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select subject.reference,\n",
    "       code.coding[0].system system, \n",
    "       code.coding[0].code code,\n",
    "       onsetDateTime\n",
    "from condition\n",
    "where in_valueset(code, 'chd')\n",
    "\"\"\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar example for the [FHIR Observation Model](https://www.hl7.org/fhir/observation.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldl_values = spark.sql(\"\"\"\n",
    "select subject.reference, \n",
    "       code.coding[0].code code,\n",
    "       valueQuantity.value,\n",
    "       effectiveDateTime\n",
    "from observation \n",
    "where in_valueset(code, 'ldl') and year(effectiveDateTime) = 2017\n",
    "\"\"\")\n",
    "\n",
    "ldl_values.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "Now that we have data in a tabular format, we can easily calculate summary statistics or plot its distribution. (Of course, in a real system the counts would be much higher, and the distribution much smoother.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldl_values.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldl_values.select(col('value').cast('float')) \\\n",
    "          .toPandas() \\\n",
    "          .plot(kind='hist', \n",
    "                bins=10, \n",
    "                figsize=(12,5), \n",
    "                legend=False) \\\n",
    "          .set_xlabel(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Cohorts\n",
    "Let's track down a group of people we want to analyze. In this case, we're interested in people:\n",
    "\n",
    "* With at least one of the following:\n",
    " * A diabetes condition\n",
    " * A pre-diabetes condition\n",
    " * An elevated HbA1c value \n",
    "* And who haven't had a wellnes visit in some time\n",
    "\n",
    "We can achieve this by simple creating dataframes with the desired subset of people. In this example, we have \"diabetes_risks\" valueset that contains diabetes-related conditions, so we just query it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_conditions = spark.sql(\"\"\"\n",
    "select id condition_id, \n",
    "       subject.reference person_ref, \n",
    "       coding.system,\n",
    "       coding.code,\n",
    "       coding.display\n",
    "from condition \n",
    "     lateral view explode(code.coding) nested as coding\n",
    "where in_valueset(code, 'diabetes_risks')\n",
    "\"\"\")\n",
    "\n",
    "diabetes_conditions.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can add further qualifications or explore the entire dataset interactively to make sure our query is producing the expected results.\n",
    "\n",
    "Now let's find people with a high hba1c:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_hba1c = spark.sql(\"\"\"\n",
    "select id observation_id,\n",
    "       subject.reference person_ref,\n",
    "       valueQuantity.value,\n",
    "       valueQuantity.unit\n",
    "from observation \n",
    "     lateral view explode(code.coding) nested as coding\n",
    "where in_valueset(code, 'hba1c') and\n",
    "      valueQuantity.value >= 6.5 and\n",
    "      status = 'final'\n",
    "\"\"\")\n",
    "\n",
    "high_hba1c.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just select the person IDs from both of the above dataframes and union them to get our complete list of people who are at risk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_risk = high_hba1c.select('person_ref') \\\n",
    "                          .union(diabetes_conditions.select('person_ref')) \\\n",
    "                          .distinct()\n",
    "        \n",
    "diabetes_risk.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find all of the wellness visits in the last couple years, which we use to exclude people from our cohort:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellness_visits = spark.sql(\"\"\"\n",
    "select subject.reference person_ref, \n",
    "       period.start encounter_start,\n",
    "       period.end encounter_start\n",
    "from encounter \n",
    "where class.code = 'WELLNESS' and\n",
    "      period.start > '2016'\n",
    "\"\"\")\n",
    "\n",
    "wellness_visits.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to exclude people who have had wellness visits, we will do an anti-join between our at-risk group and the visits dataframe we created. The end result is a simple table with the required people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_without_wellness = diabetes_risk.join(wellness_visits, \n",
    "                                               ['person_ref'], \n",
    "                                               'left_anti')\n",
    "\n",
    "diabetes_without_wellness.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Time Series\n",
    "We've looked at some general queries, but a number of machine learning and other analysis works best if we can create a time series from our data. We'll start with some (arbitrary) observation values by month.\n",
    "\n",
    "Except for the *in_valueset* method, all of this is standard SQL, leverage Apache Spark's support of nested structures. The \"group by\" clause defines the time periods to aggregate by, and aggregation functions like *avg* or *max* will conditionally include values that satisfy the nested *if* expression.\n",
    "\n",
    "Advanced SQL users can also leverage window functions over this data. See the Spark documentation for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = spark.sql(\"\"\"\n",
    "select subject.reference patient_id,\n",
    "       year(effectiveDateTime) year,\n",
    "       month(effectiveDateTime) month,\n",
    "       \n",
    "       avg(if(in_valueset(code, 'hba1c'), \n",
    "              valueQuantity.value, \n",
    "              null)) avg_hba1c_level,       \n",
    "\n",
    "       avg(if(in_valueset(code, 'ldl'), \n",
    "              valueQuantity.value, \n",
    "              null)) avg_ldl,\n",
    "\n",
    "       avg(if(in_valueset(code, 'hdl'), \n",
    "              valueQuantity.value, \n",
    "              null)) avg_hdl,\n",
    "                                          \n",
    "       max(if(in_valueset(code, 'triglycerides'), \n",
    "              valueQuantity.value, \n",
    "              null)) max_triglycerides\n",
    "              \n",
    "from observation\n",
    "where effectiveDateTime >= '2013-01-01' and\n",
    "      effectiveDateTime < '2018-01-01'\n",
    "group by subject.reference, \n",
    "         year(effectiveDateTime), \n",
    "         month(effectiveDateTime)\n",
    "order by patient_id, year, month \n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a similar aggregation for conditions. We can include an arbitrary number of FHIR resources using this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = spark.sql(\"\"\"\n",
    "select subject.reference patient_id,\n",
    "       year(onsetDateTime) year,\n",
    "       month(onsetDateTime) month,\n",
    "\n",
    "       max(if(in_valueset(code, 'hypertension'), \n",
    "              true, \n",
    "              false)) hypertension,\n",
    "              \n",
    "       max(if(in_valueset(code, 'mi'), \n",
    "              true, \n",
    "              false)) mi,\n",
    "              \n",
    "       max(if(in_valueset(code, 'chd'), \n",
    "              true, \n",
    "              false)) chd             \n",
    "              \n",
    "from condition\n",
    "where onsetDateTime >= '2013-01-01' and\n",
    "      onsetDateTime < '2018-01-01'\n",
    "group by subject.reference, \n",
    "         year(onsetDateTime), \n",
    "         month(onsetDateTime)\n",
    "order by patient_id, year, month         \n",
    "\"\"\").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we grab some demographics data about hte patient and join it to the observations and conditions data frames loaded above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = spark.sql(\"\"\"\n",
    "select id patient_id, birthDate from patient\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_patient = patients \\\n",
    "                 .join(observations, 'patient_id', 'left_outer') \\\n",
    "                 .join(conditions, ['patient_id', 'year', 'month'], 'left_outer') \\\n",
    "                 .where(col('year').isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Let's take a look at our handiwork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_patient.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to a table \n",
    "Now we will save the results of our engineering exercise to a table for others to use. This may be handing off our dataset to an analyst or data scientist for a specific use case, or just keeping a copy for ourselves so we don't need to reproduce it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('create database if not exists my_analysis_work')\n",
    "\n",
    "# Only needed so this notebook can be run repeatedly\n",
    "spark.sql('drop table if exists my_analysis_work.example_timeseries')\n",
    "\n",
    "joined_patient.write.saveAsTable('my_analysis_work.example_timeseries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query the table we just saved and see the expected results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from my_analysis_work.example_timeseries limit 10').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "Such a simple, tabular model suited for our use case is easily consumed for a variety of analysis. Let's take a look at a few:\n",
    "\n",
    "### Descriptive Statistics\n",
    "Machine Learning gets all of the attention, but a lot of problems can be solved with descriptive statistics. Fortunately Apache Spark offers a rich package for this. Here's a single line to compute the correlation between LDL and triglycerides in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_patient.corr('avg_ldl', 'max_triglycerides')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing Functions\n",
    "Spark [Windowing Functions](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html) can be easily used with this format to simplify making predictions about future state. \n",
    "\n",
    "Here we simplly add a column indicating whether the next time slot for a patient has a Coronary Heart Disease diagnosis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "window = Window.partitionBy('patient_id').orderBy('year', 'month')\n",
    "\n",
    "predict_chd = joined_patient.withColumn('next_chd', func.lead('chd').over(window))\n",
    "\n",
    "predict_chd.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "Finally, these data engineering tasks create a solid foundation for Machine Learning. We don't have time in this tutorial to go into depth here, but [Spark ML](https://spark.apache.org/docs/2.3.0/ml-pipeline.html) has a number of great references and will be very familiar to anyone familiar with Python's scikit-learn library. Users can also save our the tabular dataset and load it into any external ML tool that better fits their needs. \n",
    "\n",
    "```python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Predict CHD based on features extracted from the pipeline\n",
    "regression = LogisticRegression(featuresCol='features', \n",
    "                                labelCol='next_chd',\n",
    "                                maxIter=10)\n",
    "\n",
    "# The pipeline contains several stages that extract and normalize features\n",
    "# from our tabluar dataset\n",
    "pipeline = Pipeline(stages=[convert_to_numeric, \n",
    "                            extract_lab_features,\n",
    "                            scaled_lab_values,\n",
    "                            assemble_feature_vector,\n",
    "                            regression])\n",
    "\n",
    "```\n",
    "Now we can train the pipeline with the table we created above:\n",
    "\n",
    "```python\n",
    "# Split our table between training and test data\n",
    "(train, test) = predict_chd.randomSplit([0.7, 0.3])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
