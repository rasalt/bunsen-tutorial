{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> HIMSS Demo - HealtheDatalab </h1>\n",
    "\n",
    "<h2> Structured Machine Learning using Tensorflow </h2>\n",
    "<hr />\n",
    "This notebook demonstrates a process to train, evaluate and deploy a ML model to CloudML. It leverages a pre-built machine learning model to predict Length of Stay in ED and inpatient care settings\n",
    "<h3>\n",
    "<br />\n",
    "<ol>\n",
    "<li> Setup Environment </li> <br />\n",
    "<li> Label generation - Generate Labels in TFRecord format </li> <br />\n",
    "<li> Generate TFSequenceExamples </li> <br />\n",
    "<li> Train and Evaluate Machine Learning Model </li> <br />\n",
    "<li> Deploy ML Model to CloudML </li>\n",
    "</ol></h3>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Setup environment</h2>\n",
    "<ul>\n",
    "    <li> Initialize variables for your environment</li>\n",
    "    <li>Please change the values for the following constants before executing rest of the cells in this notebook</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GCP_PROJECT = 'dp-workspace'\n",
    "GCS_BUCKET = 'gs://cluster10-bkt'\n",
    "LABELS_JOB = 'bundlesTolabels'\n",
    "SEQEX_JOB = 'gen_seqex'\n",
    "STAGING_LOCATION = GCS_BUCKET+'/staging'\n",
    "TEMP_LOCATION = GCS_BUCKET+'/temp'\n",
    "RUNNER = 'DirectRunner'\n",
    "TF_RECORD_BUNDLES = GCS_BUCKET+'/synthea/bundles/bundles*'\n",
    "os.putenv(\"BUNDLES_IN_GCS\", TF_RECORD_BUNDLES)\n",
    "LABELS_PATH = GCS_BUCKET+'/synthea/labels/label'\n",
    "TF_RECORD_LABELS = GCS_BUCKET+'/synthea/labels/label-00000-of-00001.tfrecords'\n",
    "os.putenv(\"LABELS_IN_GCS\", TF_RECORD_LABELS)\n",
    "SEQEX_PATH = GCS_BUCKET+'/synthea/seqex/seqex'\n",
    "TF_RECORD_SEQEX = GCS_BUCKET+'/synthea/seqex/seqex*'\n",
    "os.putenv(\"SEQEX_IN_GCS\", TF_RECORD_SEQEX)\n",
    "MODEL_PATH = GCS_BUCKET+'/synthea/model/'\n",
    "os.putenv(\"MODEL_IN_GCS\", MODEL_PATH+\"*\")\n",
    "TRAINING_DATASET = GCS_BUCKET+'/synthea/seqex/seqex-00000-of-00002.tfrecords'\n",
    "VALIDATION_DATASET = GCS_BUCKET+'/synthea/seqex/seqex-00001-of-00002.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Label generation - Generate Labels in TFRecord format </h2>\n",
    "<ul>\n",
    "    <li>The next few cells generates labels from bundles in TFRecord format</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine GCS bucket that holds the bundels in TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l ${BUNDLES_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete labels generated from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil rm ${LABELS_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "import apache_beam as beam\n",
    "from tensorflow.core.example import example_pb2\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from proto.stu3 import google_extensions_pb2\n",
    "from proto.stu3 import resources_pb2\n",
    "from proto.stu3 import version_config_pb2\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from py.google.fhir.labels import label\n",
    "from py.google.fhir.labels import bundle_to_label\n",
    "from py.google.fhir.seqex import bundle_to_seqex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set options needed to initialize the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = GCP_PROJECT\n",
    "google_cloud_options.job_name = LABELS_JOB\n",
    "google_cloud_options.staging_location = STAGING_LOCATION\n",
    "google_cloud_options.temp_location = TEMP_LOCATION\n",
    "options.view_as(StandardOptions).runner = RUNNER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the pipeline to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "bundles = p | 'read' >> beam.io.ReadFromTFRecord(\n",
    "    TF_RECORD_BUNDLES, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "\n",
    "labels = bundles | 'BundleToLabel' >> beam.ParDo(\n",
    "    bundle_to_label.LengthOfStayRangeLabelAt24HoursFn(for_synthea=True))\n",
    "\n",
    "_ = labels | beam.io.WriteToTFRecord(\n",
    "    LABELS_PATH,\n",
    "    coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel),\n",
    "    file_name_suffix='.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "p.run().wait_until_finish()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the location in GCS where the generated labels have been stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l ${LABELS_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Generate TFSequenceExamples</h2>\n",
    "<ul>\n",
    "    <li>The next few cell generates Tensorflow sequence examples</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete Sequence Examples generated from previous runs and regenerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil rm ${SEQEX_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the pipeline to generate Sequence Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_seqex import _get_version_config \n",
    "\n",
    "google_cloud_options.job_name = SEQEX_JOB\n",
    "p1 = beam.Pipeline(options=options)\n",
    "\n",
    "version_config = _get_version_config(\"/usr/local/fhir/proto/stu3/version_config.textproto\")\n",
    "\n",
    "keyed_bundles = ( \n",
    "    p1 \n",
    "    | 'readBundles' >> beam.io.ReadFromTFRecord(\n",
    "        TF_RECORD_BUNDLES, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "    | 'KeyBundlesByPatientId' >> beam.ParDo(\n",
    "        bundle_to_seqex.KeyBundleByPatientIdFn()))\n",
    "\n",
    "event_labels = ( \n",
    "    p1 | 'readEventLabels' >> beam.io.ReadFromTFRecord(\n",
    "        TF_RECORD_LABELS,\n",
    "        coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel)))\n",
    "\n",
    "keyed_event_labels = bundle_to_seqex.CreateTriggerLabelsPairLists(\n",
    "    event_labels)\n",
    "\n",
    "bundles_and_labels = bundle_to_seqex.CreateBundleAndLabels(\n",
    "    keyed_bundles, keyed_event_labels)\n",
    "_ = ( \n",
    "    bundles_and_labels\n",
    "    | 'Reshuffle1' >> beam.Reshuffle()\n",
    "    | 'GenerateSeqex' >> beam.ParDo(\n",
    "        bundle_to_seqex.BundleAndLabelsToSeqexDoFn(\n",
    "            version_config=version_config,\n",
    "            enable_attribution=False,\n",
    "            generate_sequence_label=False))\n",
    "    | 'Reshuffle2' >> beam.Reshuffle()\n",
    "    | 'WriteSeqex' >> beam.io.WriteToTFRecord(\n",
    "        SEQEX_PATH,\n",
    "        coder=beam.coders.ProtoCoder(example_pb2.SequenceExample),\n",
    "        file_name_suffix='.tfrecords',\n",
    "        num_shards=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the pipeline to generate Sequence Examples. This step takes more than 5 minuites to run so have patience!!!\n",
    "We are creating two shards of examples. One for training and another one for evaluation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "p1.run().wait_until_finish()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the location in GCS where the generated Sequence Examples have been stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l ${SEQEX_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Train and Evaluate ML Model</h2>\n",
    "<ul>\n",
    "    <li>The next few cell demonstrate the process to train a ML Model using the training data set created in Step 3</li>\n",
    "    <li>Training requires sequence examples in TFRecord format</li>\n",
    "    <li>Trained ML model will be stored in Google Cloud Storage </li>\n",
    "    <li>Model will be evaluated and the evaluation output will be printed</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete previously trained model and retrain it with the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil -m rm ${MODEL_IN_GCS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from training_utils import create_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_KEY_PREFIX = 'c-'\n",
    "SEQUENCE_KEY_PREFIX = 's-'\n",
    "AGE_KEY = 'Patient.ageInYears'\n",
    "LABEL_VALUES = ['less_or_equal_3', '3_7', '7_14', 'above_14']\n",
    "\n",
    "\n",
    "def _example_index_to_sparse_index(example_indices, batch_size):\n",
    "  \"\"\"Creates a sparse index tensor from a list of example indices.\n",
    "\n",
    "  For example, this would do the transformation:\n",
    "  [0, 0, 0, 1, 3, 3] -> [[0,0], [0,1], [0,2], [1,0], [3,0], [3,1]]\n",
    "\n",
    "  The second column of the output tensor is the running count of the occurrences\n",
    "  of that example index.\n",
    "\n",
    "  Args:\n",
    "    example_indices: A sorted 1D Tensor with example indices.\n",
    "    batch_size: The batch_size. Could be larger than max(example_indices) if the\n",
    "      last examples of the batch do not have the feature present.\n",
    "  Returns:\n",
    "    The sparse index tensor.\n",
    "    The maxmium length of a row in this tensor.\n",
    "  \"\"\"\n",
    "  binned_counts = tf.bincount(example_indices, minlength=batch_size)\n",
    "  max_len = tf.to_int64(tf.reduce_max(binned_counts))\n",
    "  return tf.where(tf.sequence_mask(binned_counts)), max_len\n",
    "\n",
    "def _dedup_tensor(sp_tensor):\n",
    "  \"\"\"Dedup values of a SparseTensor along each row.\n",
    "\n",
    "  Args:\n",
    "    sp_tensor: A 2D SparseTensor to be deduped.\n",
    "  Returns:\n",
    "    A deduped SparseTensor of shape [batch_size, max_len], where max_len is\n",
    "    the maximum number of unique values for a row in the Tensor.\n",
    "  \"\"\"\n",
    "  string_batch_index = tf.as_string(sp_tensor.indices[:, 0])\n",
    "\n",
    "  # tf.unique only works on 1D tensors. To avoid deduping across examples,\n",
    "  # prepend each feature value with the example index. This requires casting\n",
    "  # to and from strings for non-string features.\n",
    "  original_dtype = sp_tensor.values.dtype\n",
    "  string_values = (\n",
    "      sp_tensor.values\n",
    "      if original_dtype == tf.string else tf.as_string(sp_tensor.values))\n",
    "  index_and_value = tf.string_join([string_batch_index, string_values],\n",
    "                                   separator='|')\n",
    "  unique_index_and_value, _ = tf.unique(index_and_value)\n",
    "\n",
    "  # split is a shape [tf.size(values), 2] tensor. The first column contains\n",
    "  # indices and the second column contains the feature value (we assume no\n",
    "  # feature contains | so we get exactly 2 values from the string split).\n",
    "  split = tf.string_split(unique_index_and_value, delimiter='|')\n",
    "  split = tf.reshape(split.values, [-1, 2])\n",
    "  string_indices = split[:, 0]\n",
    "  values = split[:, 1]\n",
    "\n",
    "  indices = tf.reshape(\n",
    "      tf.string_to_number(string_indices, out_type=tf.int32), [-1])\n",
    "  if original_dtype != tf.string:\n",
    "    values = tf.string_to_number(values, out_type=original_dtype)\n",
    "  values = tf.reshape(values, [-1])\n",
    "  # Convert example indices into SparseTensor indices, e.g.\n",
    "  # [0, 0, 0, 1, 3, 3] -> [[0,0], [0,1], [0,2], [1,0], [3,0], [3,1]]\n",
    "  batch_size = tf.to_int32(sp_tensor.dense_shape[0])\n",
    "  new_indices, max_len = _example_index_to_sparse_index(indices, batch_size)\n",
    "  return tf.SparseTensor(\n",
    "      indices=tf.to_int64(new_indices),\n",
    "      values=values,\n",
    "      dense_shape=[tf.to_int64(batch_size), max_len])\n",
    "\n",
    "def get_input_fn(mode,\n",
    "                 input_pattern,\n",
    "                 dedup,\n",
    "                 time_windows,\n",
    "                 include_age,\n",
    "                 categorical_context_features,\n",
    "                 sequence_features,\n",
    "                 time_crossed_features,\n",
    "                 batch_size,\n",
    "                 shuffle=True):\n",
    "  \"\"\"Creates an input function to an estimator.\n",
    "\n",
    "  Args:\n",
    "    mode: The execution mode, as defined in tf.estimator.ModeKeys.\n",
    "    input_pattern: Input data pattern in TFRecord format containing\n",
    "      tf.SequenceExamples.\n",
    "    dedup: Whether to remove duplicate values.\n",
    "    time_windows: List of time windows - we bucket all sequence features by\n",
    "      their age into buckets [time_windows[i], time_windows[i+1]).\n",
    "    include_age: Whether to include the age_in_years as a feature.\n",
    "    categorical_context_features: List of string context features that are valid\n",
    "      keys in the tf.SequenceExample.\n",
    "    sequence_features: List of sequence features (strings) that are valid keys\n",
    "      in the tf.SequenceExample.\n",
    "    time_crossed_features: List of list of sequence features (strings) that\n",
    "      should be crossed at each step along the time dimension.\n",
    "    batch_size: The size of the batch when reading in data.\n",
    "    shuffle: Whether to shuffle the examples.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns a dictionary of features and the target labels.\n",
    "  \"\"\"\n",
    "\n",
    "  def input_fn():\n",
    "    \"\"\"Supplies input to our model.\n",
    "\n",
    "    This function supplies input to our model, where this input is a\n",
    "    function of the mode. For example, we supply different data if\n",
    "    we're performing training versus evaluation.\n",
    "\n",
    "    Returns:\n",
    "      A tuple consisting of 1) a dictionary of tensors whose keys are\n",
    "      the feature names, and 2) a tensor of target labels if the mode\n",
    "      is not INFER (and None, otherwise).\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_features_config = dict()\n",
    "    for feature in sequence_features:\n",
    "      dtype = tf.string\n",
    "      if feature == 'Observation.value.quantity.value':\n",
    "        dtype = tf.float32\n",
    "      sequence_features_config[feature] = tf.VarLenFeature(dtype)\n",
    "\n",
    "    sequence_features_config['eventId'] = tf.FixedLenSequenceFeature(\n",
    "        [], tf.int64, allow_missing=False)\n",
    "    for cross in time_crossed_features:\n",
    "      for feature in cross:\n",
    "        dtype = tf.string\n",
    "        if feature == 'Observation.value.quantity.value':\n",
    "          dtype = tf.float32\n",
    "        sequence_features_config[feature] = tf.VarLenFeature(dtype)\n",
    "    context_features_config = dict()\n",
    "    if include_age:\n",
    "      context_features_config['timestamp'] = tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=-1)\n",
    "      context_features_config['Patient.birthDate'] = tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=-1)\n",
    "    context_features_config['sequenceLength'] = tf.FixedLenFeature(\n",
    "        [], tf.int64, default_value=-1)\n",
    "\n",
    "    for context_feature in categorical_context_features:\n",
    "      context_features_config[context_feature] = tf.VarLenFeature(tf.string)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      context_features_config['label.length_of_stay_range.class'] = (\n",
    "          tf.FixedLenFeature([], tf.string, default_value='MISSING'))\n",
    "\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    num_epochs = None if is_training else 1\n",
    "\n",
    "    with tf.name_scope('read_batch'):\n",
    "      file_names = [input_pattern]\n",
    "      files = tf.data.Dataset.list_files(file_names)\n",
    "      if shuffle:\n",
    "        files = files.shuffle(buffer_size=len(file_names))\n",
    "      dataset = (files\n",
    "                 .apply(tf.contrib.data.parallel_interleave(\n",
    "                     tf.data.TFRecordDataset, cycle_length=10))\n",
    "                 .repeat(num_epochs))\n",
    "      if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100)\n",
    "      dataset = dataset.batch(batch_size)\n",
    "\n",
    "      def _parse_fn(serialized_examples):\n",
    "        context, sequence, _ = tf.io.parse_sequence_example(\n",
    "            serialized_examples,\n",
    "            context_features=context_features_config,\n",
    "            sequence_features=sequence_features_config,\n",
    "            name='parse_sequence_example')\n",
    "        return context, sequence\n",
    "\n",
    "      dataset = dataset.map(_parse_fn, num_parallel_calls=8)\n",
    "\n",
    "      def _process(context, sequence):\n",
    "        \"\"\"Supplies input to our model.\n",
    "\n",
    "        This function supplies input to our model after parsing.\n",
    "\n",
    "        Args:\n",
    "          context: The dictionary from key to (Sparse)Tensors with context\n",
    "            features\n",
    "          sequence: The dictionary from key to (Sparse)Tensors with sequence\n",
    "            features\n",
    "\n",
    "        Returns:\n",
    "          A tuple consisting of 1) a dictionary of tensors whose keys are\n",
    "          the feature names, and 2) a tensor of target labels if the mode\n",
    "          is not INFER (and None, otherwise).\n",
    "        \"\"\"\n",
    "        # Combine into a single dictionary.\n",
    "        feature_map = {}\n",
    "        # Add age if requested.\n",
    "        if include_age:\n",
    "          age_in_seconds = (\n",
    "              context['timestamp'] -\n",
    "              context.pop('Patient.birthDate'))\n",
    "          age_in_years = tf.to_float(age_in_seconds) / (60 * 60 * 24 * 365.0)\n",
    "          feature_map[CONTEXT_KEY_PREFIX + AGE_KEY] = age_in_years\n",
    "\n",
    "        sequence_length = context.pop('sequenceLength')\n",
    "        # Cross the requested features.\n",
    "        for cross in time_crossed_features:\n",
    "          # The features may be missing at different rates - we take the union\n",
    "          # of the indices supplying defaults.\n",
    "          extended_features = dict()\n",
    "          dense_shape = tf.concat(\n",
    "              [[tf.shape(sequence_length)[0]], [tf.reduce_max(sequence_length)],\n",
    "               tf.constant([1], dtype=tf.int64)],\n",
    "              axis=0)\n",
    "          for i, feature in enumerate(cross):\n",
    "            sp_tensor = sequence[feature]\n",
    "            additional_indices = []\n",
    "            covered_indices = sp_tensor.indices\n",
    "            for j, other_feature in enumerate(cross):\n",
    "              if i != j:\n",
    "                additional_indices.append(\n",
    "                    tf.sets.set_difference(\n",
    "                        tf.sparse_reorder(\n",
    "                            tf.SparseTensor(\n",
    "                                indices=sequence[other_feature].indices,\n",
    "                                values=tf.zeros([\n",
    "                                    tf.shape(sequence[other_feature].indices)[0]\n",
    "                                ],\n",
    "                                                dtype=tf.int32),\n",
    "                                dense_shape=dense_shape)),\n",
    "                        tf.sparse_reorder(\n",
    "                            tf.SparseTensor(\n",
    "                                indices=covered_indices,\n",
    "                                values=tf.zeros([tf.shape(covered_indices)[0]],\n",
    "                                                dtype=tf.int32),\n",
    "                                dense_shape=dense_shape))).indices)\n",
    "                covered_indices = tf.concat(\n",
    "                    [sp_tensor.indices] + additional_indices, axis=0)\n",
    "\n",
    "            additional_indices = tf.concat(additional_indices, axis=0)\n",
    "\n",
    "            # Supply defaults for all other indices.\n",
    "            default = tf.tile(\n",
    "                tf.constant(['n/a']),\n",
    "                multiples=[tf.shape(additional_indices)[0]])\n",
    "\n",
    "            string_value = (\n",
    "                tf.as_string(sp_tensor.values)\n",
    "                if sp_tensor.values.dtype != tf.string else sp_tensor.values)\n",
    "\n",
    "            extended_features[feature] = tf.sparse_reorder(\n",
    "                tf.SparseTensor(\n",
    "                    indices=tf.concat([sp_tensor.indices, additional_indices],\n",
    "                                      axis=0),\n",
    "                    values=tf.concat([string_value, default], axis=0),\n",
    "                    dense_shape=dense_shape))\n",
    "\n",
    "          new_values = tf.string_join(\n",
    "              [extended_features[f].values for f in cross], separator='-')\n",
    "          crossed_sp_tensor = tf.sparse_reorder(\n",
    "              tf.SparseTensor(\n",
    "                  indices=extended_features[cross[0]].indices,\n",
    "                  values=new_values,\n",
    "                  dense_shape=extended_features[cross[0]].dense_shape))\n",
    "          sequence['_'.join(cross)] = crossed_sp_tensor\n",
    "        # Remove unwanted features that are used in the cross but should not be\n",
    "        # considered outside the cross.\n",
    "        for cross in time_crossed_features:\n",
    "          for feature in cross:\n",
    "            if feature not in sequence_features and feature in sequence:\n",
    "              del sequence[feature]\n",
    "\n",
    "        # Flatten sparse tensor to compute event age. This dense tensor also\n",
    "        # contains padded values. These will not be used when gathering elements\n",
    "        # from the dense tensor since each sparse feature won't have a value\n",
    "        # defined for the padding.\n",
    "        padded_event_age = (\n",
    "            # Broadcast current time along sequence dimension.\n",
    "            tf.expand_dims(context.pop('timestamp'), 1)\n",
    "            # Subtract time of events.\n",
    "            - sequence.pop('eventId'))\n",
    "\n",
    "        for i in range(len(time_windows) - 1):\n",
    "          max_age = time_windows[i]\n",
    "          min_age = time_windows[i+1]\n",
    "          padded_in_time_window = tf.logical_and(padded_event_age <= max_age,\n",
    "                                                 padded_event_age > min_age)\n",
    "\n",
    "          for k, v in sequence.items():\n",
    "            # For each sparse feature entry, look up whether it is in the time\n",
    "            # window or not.\n",
    "            in_time_window = tf.gather_nd(padded_in_time_window,\n",
    "                                          v.indices[:, 0:2])\n",
    "            v = tf.sparse_retain(v, in_time_window)\n",
    "            sp_tensor = tf.sparse_reshape(v, [v.dense_shape[0], -1])\n",
    "            if dedup:\n",
    "              sp_tensor = _dedup_tensor(sp_tensor)\n",
    "\n",
    "            feature_map[SEQUENCE_KEY_PREFIX + k +\n",
    "                        '-til-%d' %min_age] = sp_tensor\n",
    "\n",
    "        for k, v in context.items():\n",
    "          feature_map[CONTEXT_KEY_PREFIX + k] = v\n",
    "        return feature_map\n",
    "\n",
    "      feature_map = (dataset\n",
    "                     # Parallelize the input processing and put it behind a\n",
    "                     # queue to increase performance by removing it from the\n",
    "                     # critical path of per-step-computation.\n",
    "                     .map(_process, num_parallel_calls=8)\n",
    "                     .prefetch(buffer_size=1)\n",
    "                     .make_one_shot_iterator()\n",
    "                     .get_next())\n",
    "      label = None\n",
    "      if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label = feature_map.pop(CONTEXT_KEY_PREFIX +\n",
    "                                'label.length_of_stay_range.class')\n",
    "      return feature_map, label\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "hparams = create_hparams()\n",
    "\n",
    "time_crossed_features = [\n",
    "        cross.split(':') for cross in hparams.time_crossed_features if cross\n",
    "    ]\n",
    "\n",
    "map_, label_ = get_input_fn(tf.estimator.ModeKeys.TRAIN, TRAINING_DATASET, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=2)()\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  map_['label'] = label_\n",
    "  print(sess.run(map_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_features = []\n",
    "seq_features_sizes = []\n",
    "hparams = create_hparams()\n",
    "\n",
    "for k, bucket_size in zip(\n",
    "    hparams.sequence_features,\n",
    "    hparams.sequence_bucket_sizes):\n",
    "  for max_age in hparams.time_windows[1:]:\n",
    "    seq_features.append(\n",
    "        tf.feature_column.categorical_column_with_hash_bucket(\n",
    "            SEQUENCE_KEY_PREFIX + k + '-til-' +\n",
    "            str(max_age), bucket_size))\n",
    "    seq_features_sizes.append(bucket_size)\n",
    "\n",
    "categorical_context_features = [\n",
    "    tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        CONTEXT_KEY_PREFIX + k, bucket_size)\n",
    "    for k, bucket_size in zip(hparams.categorical_context_features,\n",
    "                              hparams.context_bucket_sizes)\n",
    "]\n",
    "discretized_context_features = []\n",
    "if hparams.include_age:\n",
    "  discretized_context_features.append(\n",
    "      tf.feature_column.bucketized_column(\n",
    "          tf.feature_column.numeric_column(CONTEXT_KEY_PREFIX + AGE_KEY),\n",
    "          boundaries=hparams.age_boundaries))\n",
    "\n",
    "optimizer = tf.train.FtrlOptimizer(\n",
    "      learning_rate=hparams.learning_rate,\n",
    "      l1_regularization_strength=hparams.l1_regularization_strength,\n",
    "      l2_regularization_strength=hparams.l2_regularization_strength)\n",
    "\n",
    "estimator = tf.estimator.LinearClassifier(\n",
    "    feature_columns=seq_features + categorical_context_features +\n",
    "    discretized_context_features,\n",
    "    n_classes=len(LABEL_VALUES),\n",
    "    label_vocabulary=LABEL_VALUES,\n",
    "    model_dir=MODEL_PATH,\n",
    "    optimizer=optimizer,\n",
    "    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_metrics_fn(labels, predictions):\n",
    "  \"\"\"Computes precsion/recall@k metrics for each class and micro-weighted.\n",
    "\n",
    "  Args:\n",
    "    labels: A string Tensor of shape [batch_size] with the true labels\n",
    "    predictions: A float Tensor of shape [batch_size, num_classes].\n",
    "\n",
    "  Returns:\n",
    "    A dictionary with metrics of precision/recall @1/2 and precision/recall per\n",
    "    class.\n",
    "  \"\"\"\n",
    "\n",
    "  label_ids = tf.contrib.lookup.index_table_from_tensor(\n",
    "      tuple(LABEL_VALUES),\n",
    "      name='class_id_lookup').lookup(labels)\n",
    "  dense_labels = tf.one_hot(label_ids, len(LABEL_VALUES))\n",
    "\n",
    "  # We convert the task to a binary one of < 7 days.\n",
    "  # 'less_or_equal_3', '3_7', '7_14', 'above_14'\n",
    "  binary_labels = label_ids < 2\n",
    "  binary_probs = tf.reduce_sum(predictions['probabilities'][:, 0:2], axis=1)\n",
    "\n",
    "  metrics_dict = {\n",
    "      'precision_at_1':\n",
    "          tf.metrics.precision_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=1),\n",
    "      'precision_at_2':\n",
    "          tf.metrics.precision_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=2),\n",
    "      'recall_at_1':\n",
    "          tf.metrics.recall_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=1),\n",
    "      'recall_at_2':\n",
    "          tf.metrics.recall_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=2),\n",
    "      'auc_roc_at_most_7d':\n",
    "          tf.metrics.auc(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs,\n",
    "              curve='ROC',\n",
    "              summation_method='careful_interpolation'),\n",
    "      'auc_pr_at_most_7d':\n",
    "          tf.metrics.auc(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs,\n",
    "              curve='PR',\n",
    "              summation_method='careful_interpolation'),\n",
    "      'precision_at_most_7d':\n",
    "          tf.metrics.precision(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs >= 0.5),\n",
    "      'recall_at_most_7d':\n",
    "          tf.metrics.recall(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs >= 0.5),\n",
    "  }\n",
    "  for i, label in enumerate(LABEL_VALUES):\n",
    "    metrics_dict['precision_%s' % label] = tf.metrics.precision_at_k(\n",
    "        labels=label_ids,\n",
    "        predictions=predictions['probabilities'],\n",
    "        k=1,\n",
    "        class_id=i)\n",
    "    metrics_dict['recall_%s' % label] = tf.metrics.recall_at_k(\n",
    "        labels=label_ids,\n",
    "        predictions=predictions['probabilities'],\n",
    "        k=1,\n",
    "        class_id=i)\n",
    "\n",
    "  return metrics_dict\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, multiclass_metrics_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(tf.estimator.ModeKeys.TRAIN, TRAINING_DATASET, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l ${MODEL_IN_GCS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input_fn = get_input_fn(tf.estimator.ModeKeys.EVAL, VALIDATION_DATASET, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=validation_input_fn, steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l ${MODEL_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Deploy ML Model to Cloud ML</h2>\n",
    "<ul>\n",
    "    <li>The trained ML Model will be deployed to CoudML for serving </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
