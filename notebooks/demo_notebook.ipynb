{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> HIMSS Demo - HealtheDatalab </h1>\n",
    "\n",
    "<h2> Structured Machine Learning using Tensorflow </h2>\n",
    "<hr />\n",
    "This notebook demonstrates a process to train, evaluate and deploy a ML model to CloudML. It leverages a pre-built machine learning model to predict Length of Stay in ED and inpatient care settings\n",
    "<h3>\n",
    "<br />\n",
    "<ol>\n",
    "<li> Access, Analize & Visualize Data using HealtheDataLab </li> <br />\n",
    "<li> Label generation - Generate Labels in TFRecord format </li> <br />\n",
    "<li> Generate TFSequenceExamples </li> <br />\n",
    "<li> Train and Evaluate Machine Learning Model </li> <br />\n",
    "<li> Deploy ML Model to CloudML </li>\n",
    "</ol></h3>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Access, Analize & Visualize Data using HealtheDataLab </h2>\n",
    "<ul>\n",
    "    <li>Import FHIR bundles (Patient's longitudinal records) into Spark Dataframes</li>\n",
    "    <li>Extract patient records into Spark Dataframes</li>\n",
    "    <li>Query and visualize patient records using Spark SQL </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from bunsen.stu3.bundles import load_from_directory, extract_entry\n",
    "from demo_utils import age\n",
    "\n",
    "# Enable Hive support for our session so we can save resources as Hive tables\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Load and cache the bundles so we don't reload them every time.\n",
    "bundles = load_from_directory(spark, 'gs://cluster-data/demo/data/synthea/fhir/').cache()\n",
    "\n",
    "# Extract patients from bundles\n",
    "patients = extract_entry(spark, bundles, 'patient')\n",
    "\n",
    "pats = patients.select('id','gender', 'birthDate', 'address.city', 'address.state', 'address.country') \n",
    "\n",
    "#pats['birthDate'] = pats['birthDate'].apply(age)\n",
    "patsDF = pats.limit(10).toPandas()\n",
    "patsDF['age'] = patsDF['birthDate'].apply(age)\n",
    "display(patsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Extract Patient Encounters into Spark Dataframes</li>\n",
    "    <li>Query and visualize Encounter records using Spark SQL </li>\n",
    "    <li>Compute Length of Stay from Encounter start and end dates. </li>\n",
    "    <li>We will use Length of Stay and other features from Patient, Observation and other records to train our linear regression model.</li>\n",
    "    <li>Our linear regression model will predict label: \"Length of Stay\"</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from demo_utils import los\n",
    "\n",
    "# Extract encounters from bundles\n",
    "encounters = extract_entry(spark, bundles, 'encounter') \n",
    "\n",
    "encs=encounters.select('subject.reference', \n",
    "                  'class.code', \n",
    "                  'period.start', \n",
    "                  'period.end') \\\n",
    "          .where(col('class.code').isin(\"inpatient\", \"emergency\"))\n",
    "\n",
    "\n",
    "encsDF = encs.limit(10).toPandas()\n",
    "encsDF['los'] = encsDF.apply(los, axis=1)\n",
    "display(encsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Label generation - Generate Labels in TFRecord format </h2>\n",
    "<ul>\n",
    "    <li>The next few cells generates labels from bundles in TFRecord format</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bundles = 'gs://cluster-data/demo/data/bundles/bundles*'\n",
    "labels_path = 'gs://cluster-data/demo/data/output/labels'\n",
    "labels = 'gs://cluster-data/demo/data/labels/train-00000-of-00001.tfrecords'\n",
    "#labels = 'gs://cluster-data/demo/data/output/labels'*'\n",
    "seqex_path = 'gs://cluster-data/demo/data/output/seqex'\n",
    "seqex_for_training = 'gs://cluster-data/demo/data/seqex/train*'\n",
    "#seqex_for_training = 'gs://cluster-data/demo/data/output/seqex*'\n",
    "seqex_for_eval = 'gs://cluster-data/demo/data/seqex/validation*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine GCS bucket that holds the bundels in TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/bundles/bundles*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete labels generated from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil rm gs://cluster-data/demo/data/output/labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "import apache_beam as beam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "from proto.stu3 import google_extensions_pb2\n",
    "from proto.stu3 import resources_pb2\n",
    "from proto.stu3 import version_config_pb2\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from py.google.fhir.labels import label\n",
    "from py.google.fhir.labels import bundle_to_label\n",
    "from py.google.fhir.seqex import bundle_to_seqex\n",
    "\n",
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = 'dp-workspace'\n",
    "google_cloud_options.job_name = 'bundlesTolabels'\n",
    "google_cloud_options.staging_location = 'gs://healthedatalab/staging'\n",
    "google_cloud_options.temp_location = 'gs://healthedatalab/temp'\n",
    "options.view_as(StandardOptions).runner = 'DirectRunner'\n",
    "\n",
    "p = beam.Pipeline(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundles = p | 'read' >> beam.io.ReadFromTFRecord(\n",
    "    input_bundles, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "\n",
    "labels = bundles | 'BundleToLabel' >> beam.ParDo(\n",
    "    bundle_to_label.LengthOfStayRangeLabelAt24HoursFn(for_synthea=True))\n",
    "\n",
    "_ = labels | beam.io.WriteToTFRecord(\n",
    "    labels_path,\n",
    "    coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel),\n",
    "    file_name_suffix='.tfrecords')\n",
    "\n",
    "p.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the output location in GCS where labels have been crearted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Generate TFSequenceExamples</h2>\n",
    "<ul>\n",
    "    <li>The next few cell generates Tensorflow sequence examples</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/labels/train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_version_config(version_config_path):\n",
    "  with open(version_config_path) as f:\n",
    "    return text_format.Parse(f.read(), version_config_pb2.VersionConfig())\n",
    "\n",
    "p1 = beam.Pipeline(options=options)\n",
    "\n",
    "version_config = _get_version_config(\"/usr/local/fhir/proto/stu3/version_config.textproto\")\n",
    "\n",
    "keyed_bundles = ( \n",
    "    p1 \n",
    "    | 'readBundles' >> beam.io.ReadFromTFRecord(\n",
    "        input_bundles, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "    | 'KeyBundlesByPatientId' >> beam.ParDo(\n",
    "        bundle_to_seqex.KeyBundleByPatientIdFn()))\n",
    "\n",
    "event_labels = ( \n",
    "    p1 | 'readEventLabels' >> beam.io.ReadFromTFRecord(\n",
    "        labels,\n",
    "        coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel)))\n",
    "\n",
    "keyed_event_labels = bundle_to_seqex.CreateTriggerLabelsPairLists(\n",
    "    event_labels)\n",
    "\n",
    "bundles_and_labels = bundle_to_seqex.CreateBundleAndLabels(\n",
    "    keyed_bundles, keyed_event_labels)\n",
    "\n",
    "_ = ( \n",
    "    bundles_and_labels\n",
    "    | 'Reshuffle1' >> beam.Reshuffle()\n",
    "    | 'GenerateSeqex' >> beam.ParDo(\n",
    "        bundle_to_seqex.BundleAndLabelsToSeqexDoFn(\n",
    "            version_config=version_config,\n",
    "            enable_attribution=False,\n",
    "            generate_sequence_label=False))\n",
    "    | 'Reshuffle2' >> beam.Reshuffle()\n",
    "    | 'WriteSeqex' >> beam.io.WriteToTFRecord(\n",
    "        seqex_path,\n",
    "        coder=beam.coders.ProtoCoder(example_pb2.SequenceExample),\n",
    "        file_name_suffix='.tfrecords',\n",
    "        num_shards=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Train and Evaluate ML Model</h2>\n",
    "<ul>\n",
    "    <li>The next few cell demonstrate the process to train a ML Model using the training data set created in Step 3</li>\n",
    "    <li>Training requires sequence examples in TFRecord format</li>\n",
    "    <li>Trained ML model will be stored in Google Cloud Storage </li>\n",
    "    <li>Model will be evaluated and the evaluation output will be printed</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'gs://healthedatalab/synthea/model/'\n",
    "train_file = 'gs://healthedatalab/sythea/seqex/seqex-00000-of-00002.tfrecords'\n",
    "validation_file = 'gs://healthedatalab/synthea/seqex/seqex-00001-of-00002.tfrecords\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Deploy ML Model to Cloud ML</h2>\n",
    "<ul>\n",
    "    <li>The trained ML Model will be deployed to CoudML for serving </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
