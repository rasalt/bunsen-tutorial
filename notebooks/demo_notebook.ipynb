{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> HIMSS Demo - HealtheDatalab </h1>\n",
    "\n",
    "<h2> Structured Machine Learning using Tensorflow </h2>\n",
    "<hr />\n",
    "This notebook demonstrates a process to train, evaluate and deploy a ML model to CloudML. It leverages a pre-built machine learning model to predict Length of Stay in ED and inpatient care settings\n",
    "<h3>\n",
    "<br />\n",
    "<ol>\n",
    "<li> Access, Analize & Visualize Data using HealtheDataLab </li> <br />\n",
    "<li> Label generation - Generate Labels in TFRecord format </li> <br />\n",
    "<li> Generate TFSequenceExamples </li> <br />\n",
    "<li> Train and Evaluate Machine Learning Model </li> <br />\n",
    "<li> Deploy ML Model to CloudML </li>\n",
    "</ol></h3>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Access, Analize & Visualize Data using HealtheDataLab </h2>\n",
    "<ul>\n",
    "    <li>Import FHIR bundles (Patient's longitudinal records) into Spark Dataframes</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from bunsen.stu3.bundles import load_from_directory, extract_entry\n",
    "from demo_utils import age\n",
    "\n",
    "# Enable Hive support for our session so we can save resources as Hive tables\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Load and cache the bundles so we don't reload them every time.\n",
    "bundles = load_from_directory(spark, 'gs://cluster-data/demo/data/synthea/fhir/').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Extract patient records into Spark Dataframes</li>\n",
    "    <li>Query and visualize patient records using Spark SQL </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthDate</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>male</td>\n",
       "      <td>1980-11-07</td>\n",
       "      <td>[Pittsfield]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:uuid:345efce8-d11d-429d-9984-6b67e31a7269</td>\n",
       "      <td>male</td>\n",
       "      <td>1952-06-04</td>\n",
       "      <td>[Harwich]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:uuid:44810270-bafe-42a4-8fc8-c229368b0058</td>\n",
       "      <td>male</td>\n",
       "      <td>1966-02-17</td>\n",
       "      <td>[Hubbardston]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urn:uuid:d6be5e17-7733-4096-b3a7-32c2a80582af</td>\n",
       "      <td>female</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>[Worcester]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urn:uuid:5c6ad3ff-99b1-47b3-92c1-a37d82a5a559</td>\n",
       "      <td>male</td>\n",
       "      <td>1961-03-13</td>\n",
       "      <td>[Methuen Town]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>urn:uuid:e3952c11-3fa2-4492-899c-bbbb8c7b6db0</td>\n",
       "      <td>male</td>\n",
       "      <td>1956-07-01</td>\n",
       "      <td>[Wareham]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>urn:uuid:665b7d87-1e8a-46f5-a2fb-6e8200f6662e</td>\n",
       "      <td>male</td>\n",
       "      <td>1952-08-05</td>\n",
       "      <td>[Hudson]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>urn:uuid:08e56bf9-7034-4b6e-8345-c61a0d910c6e</td>\n",
       "      <td>female</td>\n",
       "      <td>1963-03-29</td>\n",
       "      <td>[Brockton]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>urn:uuid:e272d8a3-73c9-4887-a457-f0d1d7cc1e44</td>\n",
       "      <td>female</td>\n",
       "      <td>2003-11-19</td>\n",
       "      <td>[Weymouth Town]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>urn:uuid:1d9e528b-18b4-4cfa-bfd4-d2eb85e9ce1b</td>\n",
       "      <td>female</td>\n",
       "      <td>1984-11-14</td>\n",
       "      <td>[Lowell]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              id  gender   birthDate  \\\n",
       "0  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354    male  1980-11-07   \n",
       "1  urn:uuid:345efce8-d11d-429d-9984-6b67e31a7269    male  1952-06-04   \n",
       "2  urn:uuid:44810270-bafe-42a4-8fc8-c229368b0058    male  1966-02-17   \n",
       "3  urn:uuid:d6be5e17-7733-4096-b3a7-32c2a80582af  female  2018-12-29   \n",
       "4  urn:uuid:5c6ad3ff-99b1-47b3-92c1-a37d82a5a559    male  1961-03-13   \n",
       "5  urn:uuid:e3952c11-3fa2-4492-899c-bbbb8c7b6db0    male  1956-07-01   \n",
       "6  urn:uuid:665b7d87-1e8a-46f5-a2fb-6e8200f6662e    male  1952-08-05   \n",
       "7  urn:uuid:08e56bf9-7034-4b6e-8345-c61a0d910c6e  female  1963-03-29   \n",
       "8  urn:uuid:e272d8a3-73c9-4887-a457-f0d1d7cc1e44  female  2003-11-19   \n",
       "9  urn:uuid:1d9e528b-18b4-4cfa-bfd4-d2eb85e9ce1b  female  1984-11-14   \n",
       "\n",
       "              city            state country age  \n",
       "0     [Pittsfield]  [Massachusetts]    [US]  38  \n",
       "1        [Harwich]  [Massachusetts]    [US]  66  \n",
       "2    [Hubbardston]  [Massachusetts]    [US]  52  \n",
       "3      [Worcester]  [Massachusetts]    [US]   0  \n",
       "4   [Methuen Town]  [Massachusetts]    [US]  57  \n",
       "5        [Wareham]  [Massachusetts]    [US]  62  \n",
       "6         [Hudson]  [Massachusetts]    [US]  66  \n",
       "7       [Brockton]  [Massachusetts]    [US]  55  \n",
       "8  [Weymouth Town]  [Massachusetts]    [US]  15  \n",
       "9         [Lowell]  [Massachusetts]    [US]  34  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract patients from bundles\n",
    "patients = extract_entry(spark, bundles, 'patient')\n",
    "\n",
    "pats = patients.select('id','gender', 'birthDate', 'address.city', 'address.state', 'address.country') \n",
    "\n",
    "#pats['birthDate'] = pats['birthDate'].apply(age)\n",
    "patsDF = pats.limit(10).toPandas()\n",
    "patsDF['age'] = patsDF['birthDate'].apply(age)\n",
    "display(patsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Extract Patient Encounters into Spark Dataframes</li>\n",
    "    <li>Query and visualize Encounter records using Spark SQL </li>\n",
    "    <li>Compute Length of Stay from Encounter start and end dates. </li>\n",
    "    <li>We will use Length of Stay and other features from Patient, Observation and other records to train our linear regression model.</li>\n",
    "    <li>Our linear regression model will predict label: \"Length of Stay\"</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>code</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1994-12-11T11:05:54-08:00</td>\n",
       "      <td>1994-12-12T11:20:54-08:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-04-06T12:05:54-07:00</td>\n",
       "      <td>1995-04-07T12:20:54-07:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-06-19T12:05:54-07:00</td>\n",
       "      <td>1995-06-20T12:05:54-07:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-08-25T12:05:54-07:00</td>\n",
       "      <td>1995-08-26T12:05:54-07:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-11-28T11:05:54-08:00</td>\n",
       "      <td>1995-11-29T11:05:54-08:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-01-18T11:05:54-08:00</td>\n",
       "      <td>1996-01-19T11:05:54-08:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-03-02T11:05:54-08:00</td>\n",
       "      <td>1996-03-03T11:20:54-08:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-04-22T12:05:54-07:00</td>\n",
       "      <td>1996-04-23T12:20:54-07:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-10-01T12:05:54-07:00</td>\n",
       "      <td>1996-10-02T12:05:54-07:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-12-17T11:05:54-08:00</td>\n",
       "      <td>1996-12-18T11:20:54-08:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       reference       code  \\\n",
       "0  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "1  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "2  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "3  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "4  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "5  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "6  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "7  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "8  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "9  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "\n",
       "                       start                        end             los  \n",
       "0  1994-12-11T11:05:54-08:00  1994-12-12T11:20:54-08:00  1 day, 0:15:00  \n",
       "1  1995-04-06T12:05:54-07:00  1995-04-07T12:20:54-07:00  1 day, 0:15:00  \n",
       "2  1995-06-19T12:05:54-07:00  1995-06-20T12:05:54-07:00  1 day, 0:00:00  \n",
       "3  1995-08-25T12:05:54-07:00  1995-08-26T12:05:54-07:00  1 day, 0:00:00  \n",
       "4  1995-11-28T11:05:54-08:00  1995-11-29T11:05:54-08:00  1 day, 0:00:00  \n",
       "5  1996-01-18T11:05:54-08:00  1996-01-19T11:05:54-08:00  1 day, 0:00:00  \n",
       "6  1996-03-02T11:05:54-08:00  1996-03-03T11:20:54-08:00  1 day, 0:15:00  \n",
       "7  1996-04-22T12:05:54-07:00  1996-04-23T12:20:54-07:00  1 day, 0:15:00  \n",
       "8  1996-10-01T12:05:54-07:00  1996-10-02T12:05:54-07:00  1 day, 0:00:00  \n",
       "9  1996-12-17T11:05:54-08:00  1996-12-18T11:20:54-08:00  1 day, 0:15:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from demo_utils import los\n",
    "\n",
    "# Extract encounters from bundles\n",
    "encounters = extract_entry(spark, bundles, 'encounter') \n",
    "\n",
    "encs=encounters.select('subject.reference', \n",
    "                  'class.code', \n",
    "                  'period.start', \n",
    "                  'period.end') \\\n",
    "          .where(col('class.code').isin(\"inpatient\", \"emergency\"))\n",
    "\n",
    "\n",
    "encsDF = encs.limit(10).toPandas()\n",
    "encsDF['los'] = encsDF.apply(los, axis=1)\n",
    "display(encsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Label generation - Generate Labels in TFRecord format </h2>\n",
    "<ul>\n",
    "    <li>The next few cells generates labels from bundles in TFRecord format</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_bundles = 'gs://cluster-data/demo/data/bundles/bundles*'\n",
    "labels_path = 'gs://cluster-data/demo/data/output/labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine GCS bucket that holds the bundels in TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32501370  2019-01-08T23:10:08Z  gs://cluster-data/demo/data/bundles/bundles-00000-of-00010.tfrecords\n",
      "  39740597  2019-01-08T23:10:08Z  gs://cluster-data/demo/data/bundles/bundles-00001-of-00010.tfrecords\n",
      "  32894855  2019-01-08T23:10:09Z  gs://cluster-data/demo/data/bundles/bundles-00002-of-00010.tfrecords\n",
      "  30817812  2019-01-08T23:10:10Z  gs://cluster-data/demo/data/bundles/bundles-00003-of-00010.tfrecords\n",
      "  33319395  2019-01-08T23:10:11Z  gs://cluster-data/demo/data/bundles/bundles-00004-of-00010.tfrecords\n",
      "  48719477  2019-01-08T23:10:11Z  gs://cluster-data/demo/data/bundles/bundles-00005-of-00010.tfrecords\n",
      "  42681976  2019-01-08T23:10:12Z  gs://cluster-data/demo/data/bundles/bundles-00006-of-00010.tfrecords\n",
      "  32319546  2019-01-08T23:10:13Z  gs://cluster-data/demo/data/bundles/bundles-00007-of-00010.tfrecords\n",
      "  49995527  2019-01-08T23:10:14Z  gs://cluster-data/demo/data/bundles/bundles-00008-of-00010.tfrecords\n",
      "  36610205  2019-01-08T23:10:14Z  gs://cluster-data/demo/data/bundles/bundles-00009-of-00010.tfrecords\n",
      "TOTAL: 10 objects, 379600760 bytes (362.02 MiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/bundles/bundles*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete labels generated from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: gs://cluster-data/demo/data/output/labels*\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil rm gs://cluster-data/demo/data/output/labels*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "import apache_beam as beam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "from proto.stu3 import google_extensions_pb2\n",
    "from proto.stu3 import resources_pb2\n",
    "from proto.stu3 import version_config_pb2\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from py.google.fhir.labels import label\n",
    "from py.google.fhir.labels import bundle_to_label\n",
    "from py.google.fhir.seqex import bundle_to_seqex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set options needed to initialize the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = 'dp-workspace'\n",
    "google_cloud_options.job_name = 'bundlesTolabels'\n",
    "google_cloud_options.staging_location = 'gs://cluster-data/staging'\n",
    "google_cloud_options.temp_location = 'gs://cluster-data/temp'\n",
    "options.view_as(StandardOptions).runner = 'DirectRunner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the pipeline to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/oauth2client/contrib/gce.py:99: UserWarning: You have requested explicit scopes to be used with a GCE service account.\n",
      "Using this argument will have no effect on the actual scopes for tokens\n",
      "requested. These scopes are set at VM instance creation time and\n",
      "can't be overridden in the request.\n",
      "\n",
      "  warnings.warn(_SCOPES_WARNING)\n"
     ]
    }
   ],
   "source": [
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "bundles = p | 'read' >> beam.io.ReadFromTFRecord(\n",
    "    input_bundles, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "\n",
    "labels = bundles | 'BundleToLabel' >> beam.ParDo(\n",
    "    bundle_to_label.LengthOfStayRangeLabelAt24HoursFn(for_synthea=True))\n",
    "\n",
    "_ = labels | beam.io.WriteToTFRecord(\n",
    "    labels_path,\n",
    "    coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel),\n",
    "    file_name_suffix='.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0124 22:25:21.014255 139660094785280 tfrecordio.py:49] Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the output location in GCS where labels have been crearted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     97285  2019-01-15T10:25:54Z  gs://cluster-data/demo/data/output/label-00000-of-00001.tfrecords\n",
      "     97285  2019-01-24T22:25:48Z  gs://cluster-data/demo/data/output/labels-00000-of-00001.tfrecords\n",
      "                                 gs://cluster-data/demo/data/output/model/\n",
      "TOTAL: 2 objects, 194570 bytes (190.01 KiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Generate TFSequenceExamples</h2>\n",
    "<ul>\n",
    "    <li>The next few cell generates Tensorflow sequence examples</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_bundles = 'gs://cluster-data/demo/data/bundles/bundles*'\n",
    "#labels = 'gs://cluster-data/demo/data/labels/train-00000-of-00001.tfrecords'\n",
    "labels = 'gs://cluster-data/demo/data/output/labels*'\n",
    "seqex_path = 'gs://cluster-data/demo/data/output/seqex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/output/labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gen_seqex",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-5b37b3aba9eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgen_seqex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_get_version_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgoogle_cloud_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bundlesTolabels'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gen_seqex"
     ]
    }
   ],
   "source": [
    "from gen_seqex import _get_version_config \n",
    "\n",
    "google_cloud_options.job_name = 'bundlesTolabels'\n",
    "p1 = beam.Pipeline(options=options)\n",
    "\n",
    "version_config = _get_version_config(\"/usr/local/fhir/proto/stu3/version_config.textproto\")\n",
    "\n",
    "keyed_bundles = ( \n",
    "    p1 \n",
    "    | 'readBundles' >> beam.io.ReadFromTFRecord(\n",
    "        input_bundles, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "    | 'KeyBundlesByPatientId' >> beam.ParDo(\n",
    "        bundle_to_seqex.KeyBundleByPatientIdFn()))\n",
    "\n",
    "event_labels = ( \n",
    "    p1 | 'readEventLabels' >> beam.io.ReadFromTFRecord(\n",
    "        labels,\n",
    "        coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel)))\n",
    "\n",
    "keyed_event_labels = bundle_to_seqex.CreateTriggerLabelsPairLists(\n",
    "    event_labels)\n",
    "\n",
    "bundles_and_labels = bundle_to_seqex.CreateBundleAndLabels(\n",
    "    keyed_bundles, keyed_event_labels)\n",
    "_ = ( \n",
    "    bundles_and_labels\n",
    "    | 'Reshuffle1' >> beam.Reshuffle()\n",
    "    | 'GenerateSeqex' >> beam.ParDo(\n",
    "        bundle_to_seqex.BundleAndLabelsToSeqexDoFn(\n",
    "            version_config=version_config,\n",
    "            enable_attribution=False,\n",
    "            generate_sequence_label=False))\n",
    "    | 'Reshuffle2' >> beam.Reshuffle()\n",
    "    | 'WriteSeqex' >> beam.io.WriteToTFRecord(\n",
    "        seqex_path,\n",
    "        coder=beam.coders.ProtoCoder(example_pb2.SequenceExample),\n",
    "        file_name_suffix='.tfrecords',\n",
    "        num_shards=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/output/seqex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Train and Evaluate ML Model</h2>\n",
    "<ul>\n",
    "    <li>The next few cell demonstrate the process to train a ML Model using the training data set created in Step 3</li>\n",
    "    <li>Training requires sequence examples in TFRecord format</li>\n",
    "    <li>Trained ML model will be stored in Google Cloud Storage </li>\n",
    "    <li>Model will be evaluated and the evaluation output will be printed</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model_path = 'gs://cluster-data/demo/data/output/model'\n",
    "train_file = 'gs://cluster-data/demo/data/seqex/train-00000-of-00010.tfrecords'\n",
    "validation_file = 'gs://cluster-data/demo/data/seqex/validation-00000-of-00010.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import create_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_KEY_PREFIX = 'c-'\n",
    "SEQUENCE_KEY_PREFIX = 's-'\n",
    "AGE_KEY = 'Patient.ageInYears'\n",
    "LABEL_VALUES = ['less_or_equal_3', '3_7', '7_14', 'above_14']\n",
    "\n",
    "\n",
    "def _example_index_to_sparse_index(example_indices, batch_size):\n",
    "  \"\"\"Creates a sparse index tensor from a list of example indices.\n",
    "\n",
    "  For example, this would do the transformation:\n",
    "  [0, 0, 0, 1, 3, 3] -> [[0,0], [0,1], [0,2], [1,0], [3,0], [3,1]]\n",
    "\n",
    "  The second column of the output tensor is the running count of the occurrences\n",
    "  of that example index.\n",
    "\n",
    "  Args:\n",
    "    example_indices: A sorted 1D Tensor with example indices.\n",
    "    batch_size: The batch_size. Could be larger than max(example_indices) if the\n",
    "      last examples of the batch do not have the feature present.\n",
    "  Returns:\n",
    "    The sparse index tensor.\n",
    "    The maxmium length of a row in this tensor.\n",
    "  \"\"\"\n",
    "  binned_counts = tf.bincount(example_indices, minlength=batch_size)\n",
    "  max_len = tf.to_int64(tf.reduce_max(binned_counts))\n",
    "  return tf.where(tf.sequence_mask(binned_counts)), max_len\n",
    "\n",
    "def _dedup_tensor(sp_tensor):\n",
    "  \"\"\"Dedup values of a SparseTensor along each row.\n",
    "\n",
    "  Args:\n",
    "    sp_tensor: A 2D SparseTensor to be deduped.\n",
    "  Returns:\n",
    "    A deduped SparseTensor of shape [batch_size, max_len], where max_len is\n",
    "    the maximum number of unique values for a row in the Tensor.\n",
    "  \"\"\"\n",
    "  string_batch_index = tf.as_string(sp_tensor.indices[:, 0])\n",
    "\n",
    "  # tf.unique only works on 1D tensors. To avoid deduping across examples,\n",
    "  # prepend each feature value with the example index. This requires casting\n",
    "  # to and from strings for non-string features.\n",
    "  original_dtype = sp_tensor.values.dtype\n",
    "  string_values = (\n",
    "      sp_tensor.values\n",
    "      if original_dtype == tf.string else tf.as_string(sp_tensor.values))\n",
    "  index_and_value = tf.string_join([string_batch_index, string_values],\n",
    "                                   separator='|')\n",
    "  unique_index_and_value, _ = tf.unique(index_and_value)\n",
    "\n",
    "  # split is a shape [tf.size(values), 2] tensor. The first column contains\n",
    "  # indices and the second column contains the feature value (we assume no\n",
    "  # feature contains | so we get exactly 2 values from the string split).\n",
    "  split = tf.string_split(unique_index_and_value, delimiter='|')\n",
    "  split = tf.reshape(split.values, [-1, 2])\n",
    "  string_indices = split[:, 0]\n",
    "  values = split[:, 1]\n",
    "\n",
    "  indices = tf.reshape(\n",
    "      tf.string_to_number(string_indices, out_type=tf.int32), [-1])\n",
    "  if original_dtype != tf.string:\n",
    "    values = tf.string_to_number(values, out_type=original_dtype)\n",
    "  values = tf.reshape(values, [-1])\n",
    "  # Convert example indices into SparseTensor indices, e.g.\n",
    "  # [0, 0, 0, 1, 3, 3] -> [[0,0], [0,1], [0,2], [1,0], [3,0], [3,1]]\n",
    "  batch_size = tf.to_int32(sp_tensor.dense_shape[0])\n",
    "  new_indices, max_len = _example_index_to_sparse_index(indices, batch_size)\n",
    "  return tf.SparseTensor(\n",
    "      indices=tf.to_int64(new_indices),\n",
    "      values=values,\n",
    "      dense_shape=[tf.to_int64(batch_size), max_len])\n",
    "\n",
    "def get_input_fn(mode,\n",
    "                 input_pattern,\n",
    "                 dedup,\n",
    "                 time_windows,\n",
    "                 include_age,\n",
    "                 categorical_context_features,\n",
    "                 sequence_features,\n",
    "                 time_crossed_features,\n",
    "                 batch_size,\n",
    "                 shuffle=True):\n",
    "  \"\"\"Creates an input function to an estimator.\n",
    "\n",
    "  Args:\n",
    "    mode: The execution mode, as defined in tf.estimator.ModeKeys.\n",
    "    input_pattern: Input data pattern in TFRecord format containing\n",
    "      tf.SequenceExamples.\n",
    "    dedup: Whether to remove duplicate values.\n",
    "    time_windows: List of time windows - we bucket all sequence features by\n",
    "      their age into buckets [time_windows[i], time_windows[i+1]).\n",
    "    include_age: Whether to include the age_in_years as a feature.\n",
    "    categorical_context_features: List of string context features that are valid\n",
    "      keys in the tf.SequenceExample.\n",
    "    sequence_features: List of sequence features (strings) that are valid keys\n",
    "      in the tf.SequenceExample.\n",
    "    time_crossed_features: List of list of sequence features (strings) that\n",
    "      should be crossed at each step along the time dimension.\n",
    "    batch_size: The size of the batch when reading in data.\n",
    "    shuffle: Whether to shuffle the examples.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns a dictionary of features and the target labels.\n",
    "  \"\"\"\n",
    "\n",
    "  def input_fn():\n",
    "    \"\"\"Supplies input to our model.\n",
    "\n",
    "    This function supplies input to our model, where this input is a\n",
    "    function of the mode. For example, we supply different data if\n",
    "    we're performing training versus evaluation.\n",
    "\n",
    "    Returns:\n",
    "      A tuple consisting of 1) a dictionary of tensors whose keys are\n",
    "      the feature names, and 2) a tensor of target labels if the mode\n",
    "      is not INFER (and None, otherwise).\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_features_config = dict()\n",
    "    for feature in sequence_features:\n",
    "      dtype = tf.string\n",
    "      if feature == 'Observation.value.quantity.value':\n",
    "        dtype = tf.float32\n",
    "      sequence_features_config[feature] = tf.VarLenFeature(dtype)\n",
    "\n",
    "    sequence_features_config['eventId'] = tf.FixedLenSequenceFeature(\n",
    "        [], tf.int64, allow_missing=False)\n",
    "    for cross in time_crossed_features:\n",
    "      for feature in cross:\n",
    "        dtype = tf.string\n",
    "        if feature == 'Observation.value.quantity.value':\n",
    "          dtype = tf.float32\n",
    "        sequence_features_config[feature] = tf.VarLenFeature(dtype)\n",
    "    context_features_config = dict()\n",
    "    if include_age:\n",
    "      context_features_config['timestamp'] = tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=-1)\n",
    "      context_features_config['Patient.birthDate'] = tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=-1)\n",
    "    context_features_config['sequenceLength'] = tf.FixedLenFeature(\n",
    "        [], tf.int64, default_value=-1)\n",
    "\n",
    "    for context_feature in categorical_context_features:\n",
    "      context_features_config[context_feature] = tf.VarLenFeature(tf.string)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      context_features_config['label.length_of_stay_range.class'] = (\n",
    "          tf.FixedLenFeature([], tf.string, default_value='MISSING'))\n",
    "\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    num_epochs = None if is_training else 1\n",
    "\n",
    "    with tf.name_scope('read_batch'):\n",
    "      file_names = [input_pattern]\n",
    "      files = tf.data.Dataset.list_files(file_names)\n",
    "      if shuffle:\n",
    "        files = files.shuffle(buffer_size=len(file_names))\n",
    "      dataset = (files\n",
    "                 .apply(tf.contrib.data.parallel_interleave(\n",
    "                     tf.data.TFRecordDataset, cycle_length=10))\n",
    "                 .repeat(num_epochs))\n",
    "      if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100)\n",
    "      dataset = dataset.batch(batch_size)\n",
    "\n",
    "      def _parse_fn(serialized_examples):\n",
    "        context, sequence, _ = tf.io.parse_sequence_example(\n",
    "            serialized_examples,\n",
    "            context_features=context_features_config,\n",
    "            sequence_features=sequence_features_config,\n",
    "            name='parse_sequence_example')\n",
    "        return context, sequence\n",
    "\n",
    "      dataset = dataset.map(_parse_fn, num_parallel_calls=8)\n",
    "\n",
    "      def _process(context, sequence):\n",
    "        \"\"\"Supplies input to our model.\n",
    "\n",
    "        This function supplies input to our model after parsing.\n",
    "\n",
    "        Args:\n",
    "          context: The dictionary from key to (Sparse)Tensors with context\n",
    "            features\n",
    "          sequence: The dictionary from key to (Sparse)Tensors with sequence\n",
    "            features\n",
    "\n",
    "        Returns:\n",
    "          A tuple consisting of 1) a dictionary of tensors whose keys are\n",
    "          the feature names, and 2) a tensor of target labels if the mode\n",
    "          is not INFER (and None, otherwise).\n",
    "        \"\"\"\n",
    "        # Combine into a single dictionary.\n",
    "        feature_map = {}\n",
    "        # Add age if requested.\n",
    "        if include_age:\n",
    "          age_in_seconds = (\n",
    "              context['timestamp'] -\n",
    "              context.pop('Patient.birthDate'))\n",
    "          age_in_years = tf.to_float(age_in_seconds) / (60 * 60 * 24 * 365.0)\n",
    "          feature_map[CONTEXT_KEY_PREFIX + AGE_KEY] = age_in_years\n",
    "\n",
    "        sequence_length = context.pop('sequenceLength')\n",
    "        # Cross the requested features.\n",
    "        for cross in time_crossed_features:\n",
    "          # The features may be missing at different rates - we take the union\n",
    "          # of the indices supplying defaults.\n",
    "          extended_features = dict()\n",
    "          dense_shape = tf.concat(\n",
    "              [[tf.shape(sequence_length)[0]], [tf.reduce_max(sequence_length)],\n",
    "               tf.constant([1], dtype=tf.int64)],\n",
    "              axis=0)\n",
    "          for i, feature in enumerate(cross):\n",
    "            sp_tensor = sequence[feature]\n",
    "            additional_indices = []\n",
    "            covered_indices = sp_tensor.indices\n",
    "            for j, other_feature in enumerate(cross):\n",
    "              if i != j:\n",
    "                additional_indices.append(\n",
    "                    tf.sets.set_difference(\n",
    "                        tf.sparse_reorder(\n",
    "                            tf.SparseTensor(\n",
    "                                indices=sequence[other_feature].indices,\n",
    "                                values=tf.zeros([\n",
    "                                    tf.shape(sequence[other_feature].indices)[0]\n",
    "                                ],\n",
    "                                                dtype=tf.int32),\n",
    "                                dense_shape=dense_shape)),\n",
    "                        tf.sparse_reorder(\n",
    "                            tf.SparseTensor(\n",
    "                                indices=covered_indices,\n",
    "                                values=tf.zeros([tf.shape(covered_indices)[0]],\n",
    "                                                dtype=tf.int32),\n",
    "                                dense_shape=dense_shape))).indices)\n",
    "                covered_indices = tf.concat(\n",
    "                    [sp_tensor.indices] + additional_indices, axis=0)\n",
    "\n",
    "            additional_indices = tf.concat(additional_indices, axis=0)\n",
    "\n",
    "            # Supply defaults for all other indices.\n",
    "            default = tf.tile(\n",
    "                tf.constant(['n/a']),\n",
    "                multiples=[tf.shape(additional_indices)[0]])\n",
    "\n",
    "            string_value = (\n",
    "                tf.as_string(sp_tensor.values)\n",
    "                if sp_tensor.values.dtype != tf.string else sp_tensor.values)\n",
    "\n",
    "            extended_features[feature] = tf.sparse_reorder(\n",
    "                tf.SparseTensor(\n",
    "                    indices=tf.concat([sp_tensor.indices, additional_indices],\n",
    "                                      axis=0),\n",
    "                    values=tf.concat([string_value, default], axis=0),\n",
    "                    dense_shape=dense_shape))\n",
    "\n",
    "          new_values = tf.string_join(\n",
    "              [extended_features[f].values for f in cross], separator='-')\n",
    "          crossed_sp_tensor = tf.sparse_reorder(\n",
    "              tf.SparseTensor(\n",
    "                  indices=extended_features[cross[0]].indices,\n",
    "                  values=new_values,\n",
    "                  dense_shape=extended_features[cross[0]].dense_shape))\n",
    "          sequence['_'.join(cross)] = crossed_sp_tensor\n",
    "        # Remove unwanted features that are used in the cross but should not be\n",
    "        # considered outside the cross.\n",
    "        for cross in time_crossed_features:\n",
    "          for feature in cross:\n",
    "            if feature not in sequence_features and feature in sequence:\n",
    "              del sequence[feature]\n",
    "\n",
    "        # Flatten sparse tensor to compute event age. This dense tensor also\n",
    "        # contains padded values. These will not be used when gathering elements\n",
    "        # from the dense tensor since each sparse feature won't have a value\n",
    "        # defined for the padding.\n",
    "        padded_event_age = (\n",
    "            # Broadcast current time along sequence dimension.\n",
    "            tf.expand_dims(context.pop('timestamp'), 1)\n",
    "            # Subtract time of events.\n",
    "            - sequence.pop('eventId'))\n",
    "\n",
    "        for i in range(len(time_windows) - 1):\n",
    "          max_age = time_windows[i]\n",
    "          min_age = time_windows[i+1]\n",
    "          padded_in_time_window = tf.logical_and(padded_event_age <= max_age,\n",
    "                                                 padded_event_age > min_age)\n",
    "\n",
    "          for k, v in sequence.items():\n",
    "            # For each sparse feature entry, look up whether it is in the time\n",
    "            # window or not.\n",
    "            in_time_window = tf.gather_nd(padded_in_time_window,\n",
    "                                          v.indices[:, 0:2])\n",
    "            v = tf.sparse_retain(v, in_time_window)\n",
    "            sp_tensor = tf.sparse_reshape(v, [v.dense_shape[0], -1])\n",
    "            if dedup:\n",
    "              sp_tensor = _dedup_tensor(sp_tensor)\n",
    "\n",
    "            feature_map[SEQUENCE_KEY_PREFIX + k +\n",
    "                        '-til-%d' %min_age] = sp_tensor\n",
    "\n",
    "        for k, v in context.items():\n",
    "          feature_map[CONTEXT_KEY_PREFIX + k] = v\n",
    "        return feature_map\n",
    "\n",
    "      feature_map = (dataset\n",
    "                     # Parallelize the input processing and put it behind a\n",
    "                     # queue to increase performance by removing it from the\n",
    "                     # critical path of per-step-computation.\n",
    "                     .map(_process, num_parallel_calls=8)\n",
    "                     .prefetch(buffer_size=1)\n",
    "                     .make_one_shot_iterator()\n",
    "                     .get_next())\n",
    "      label = None\n",
    "      if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label = feature_map.pop(CONTEXT_KEY_PREFIX +\n",
    "                                'label.length_of_stay_range.class')\n",
    "      return feature_map, label\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "hparams = create_hparams()\n",
    "\n",
    "time_crossed_features = [\n",
    "        cross.split(':') for cross in hparams.time_crossed_features if cross\n",
    "    ]\n",
    "\n",
    "map_, label_ = get_input_fn(tf.estimator.ModeKeys.TRAIN, train_file, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=2)()\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  map_['label'] = label_\n",
    "  print(sess.run(map_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_features = []\n",
    "seq_features_sizes = []\n",
    "hparams = create_hparams()\n",
    "\n",
    "for k, bucket_size in zip(\n",
    "    hparams.sequence_features,\n",
    "    hparams.sequence_bucket_sizes):\n",
    "  for max_age in hparams.time_windows[1:]:\n",
    "    seq_features.append(\n",
    "        tf.feature_column.categorical_column_with_hash_bucket(\n",
    "            SEQUENCE_KEY_PREFIX + k + '-til-' +\n",
    "            str(max_age), bucket_size))\n",
    "    seq_features_sizes.append(bucket_size)\n",
    "\n",
    "categorical_context_features = [\n",
    "    tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        CONTEXT_KEY_PREFIX + k, bucket_size)\n",
    "    for k, bucket_size in zip(hparams.categorical_context_features,\n",
    "                              hparams.context_bucket_sizes)\n",
    "]\n",
    "discretized_context_features = []\n",
    "if hparams.include_age:\n",
    "  discretized_context_features.append(\n",
    "      tf.feature_column.bucketized_column(\n",
    "          tf.feature_column.numeric_column(CONTEXT_KEY_PREFIX + AGE_KEY),\n",
    "          boundaries=hparams.age_boundaries))\n",
    "\n",
    "optimizer = tf.train.FtrlOptimizer(\n",
    "      learning_rate=hparams.learning_rate,\n",
    "      l1_regularization_strength=hparams.l1_regularization_strength,\n",
    "      l2_regularization_strength=hparams.l2_regularization_strength)\n",
    "\n",
    "estimator = tf.estimator.LinearClassifier(\n",
    "    feature_columns=seq_features + categorical_context_features +\n",
    "    discretized_context_features,\n",
    "    n_classes=len(LABEL_VALUES),\n",
    "    label_vocabulary=LABEL_VALUES,\n",
    "    model_dir=model_path,\n",
    "    optimizer=optimizer,\n",
    "    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_metrics_fn(labels, predictions):\n",
    "  \"\"\"Computes precsion/recall@k metrics for each class and micro-weighted.\n",
    "\n",
    "  Args:\n",
    "    labels: A string Tensor of shape [batch_size] with the true labels\n",
    "    predictions: A float Tensor of shape [batch_size, num_classes].\n",
    "\n",
    "  Returns:\n",
    "    A dictionary with metrics of precision/recall @1/2 and precision/recall per\n",
    "    class.\n",
    "  \"\"\"\n",
    "\n",
    "  label_ids = tf.contrib.lookup.index_table_from_tensor(\n",
    "      tuple(LABEL_VALUES),\n",
    "      name='class_id_lookup').lookup(labels)\n",
    "  dense_labels = tf.one_hot(label_ids, len(LABEL_VALUES))\n",
    "\n",
    "  # We convert the task to a binary one of < 7 days.\n",
    "  # 'less_or_equal_3', '3_7', '7_14', 'above_14'\n",
    "  binary_labels = label_ids < 2\n",
    "  binary_probs = tf.reduce_sum(predictions['probabilities'][:, 0:2], axis=1)\n",
    "\n",
    "  metrics_dict = {\n",
    "      'precision_at_1':\n",
    "          tf.metrics.precision_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=1),\n",
    "      'precision_at_2':\n",
    "          tf.metrics.precision_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=2),\n",
    "      'recall_at_1':\n",
    "          tf.metrics.recall_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=1),\n",
    "      'recall_at_2':\n",
    "          tf.metrics.recall_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=2),\n",
    "      'auc_roc_at_most_7d':\n",
    "          tf.metrics.auc(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs,\n",
    "              curve='ROC',\n",
    "              summation_method='careful_interpolation'),\n",
    "      'auc_pr_at_most_7d':\n",
    "          tf.metrics.auc(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs,\n",
    "              curve='PR',\n",
    "              summation_method='careful_interpolation'),\n",
    "      'precision_at_most_7d':\n",
    "          tf.metrics.precision(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs >= 0.5),\n",
    "      'recall_at_most_7d':\n",
    "          tf.metrics.recall(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs >= 0.5),\n",
    "  }\n",
    "  for i, label in enumerate(LABEL_VALUES):\n",
    "    metrics_dict['precision_%s' % label] = tf.metrics.precision_at_k(\n",
    "        labels=label_ids,\n",
    "        predictions=predictions['probabilities'],\n",
    "        k=1,\n",
    "        class_id=i)\n",
    "    metrics_dict['recall_%s' % label] = tf.metrics.recall_at_k(\n",
    "        labels=label_ids,\n",
    "        predictions=predictions['probabilities'],\n",
    "        k=1,\n",
    "        class_id=i)\n",
    "\n",
    "  return metrics_dict\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, multiclass_metrics_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(tf.estimator.ModeKeys.TRAIN, train_file, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input_fn = get_input_fn(tf.estimator.ModeKeys.EVAL, validation_file, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=validation_input_fn, steps=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Deploy ML Model to Cloud ML</h2>\n",
    "<ul>\n",
    "    <li>The trained ML Model will be deployed to CoudML for serving </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
