{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> HIMSS Demo - HealtheDatalab </h1>\n",
    "\n",
    "<h2> Structured Machine Learning using Tensorflow </h2>\n",
    "<hr />\n",
    "This notebook demonstrates a process to train, evaluate and deploy a ML model to CloudML. It leverages a pre-built machine learning model to predict Length of Stay in ED and inpatient care settings\n",
    "<h3>\n",
    "<br />\n",
    "<ol>\n",
    "<li> Access, Analize & Visualize Data using HealtheDataLab </li> <br />\n",
    "<li> Label generation - Generate Labels in TFRecord format </li> <br />\n",
    "<li> Generate TFSequenceExamples </li> <br />\n",
    "<li> Train and Evaluate Machine Learning Model </li> <br />\n",
    "<li> Deploy ML Model to CloudML </li>\n",
    "</ol></h3>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 0. Initialize variables for your environment</h2>\n",
    "<ul>\n",
    "    <li>Please change the values for the following constants before executing rest of the cells in this notebook</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GCP_PROJECT = 'dp-workspace'\n",
    "GCS_BUCKET = 'gs://cluster-data'\n",
    "LABELS_JOB = 'bundlesTolabels'\n",
    "SEQEX_JOB = 'gen_seqex'\n",
    "STAGING_LOCATION = GCS_BUCKET+'/staging'\n",
    "TEMP_LOCATION = GCS_BUCKET+'/temp'\n",
    "RUNNER = 'DirectRunner'\n",
    "FHIR_JSON_BUNDLES = GCS_BUCKET+'/demo/data/synthea/fhir/'\n",
    "TF_RECORD_BUNDLES = GCS_BUCKET+'/demo/data/bundles/bundles*'\n",
    "os.putenv(\"BUNDLES_IN_GCS\", TF_RECORD_BUNDLES)\n",
    "LABELS_PATH = GCS_BUCKET+'/demo/data/output/labels'\n",
    "TF_RECORD_LABELS = GCS_BUCKET+'/demo/data/output/labels*'\n",
    "os.putenv(\"LABELS_IN_GCS\", TF_RECORD_LABELS)\n",
    "SEQEX_PATH = GCS_BUCKET+'/demo/data/output/seqex'\n",
    "TF_RECORD_SEQEX = GCS_BUCKET+'/demo/data/output/seqex*'\n",
    "os.putenv(\"SEQEX_IN_GCS\", TF_RECORD_SEQEX)\n",
    "MODEL_PATH = GCS_BUCKET+'/demo/data/output/model'\n",
    "os.putenv(\"MODEL_IN_GCS\", MODEL_PATH)\n",
    "TRAINING_DATASET = GCS_BUCKET+'/demo/data/seqex/train-00000-of-00010.tfrecords'\n",
    "VALIDATION_DATASET = GCS_BUCKET+'/demo/data/seqex/validation-00000-of-00010.tfrecords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Access, Analize & Visualize Data using HealtheDataLab </h2>\n",
    "<ul>\n",
    "    <li>Import FHIR bundles (Patient's longitudinal records) into Spark Dataframes</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from bunsen.stu3.bundles import load_from_directory, extract_entry\n",
    "from demo_utils import age\n",
    "\n",
    "# Enable Hive support for our session so we can save resources as Hive tables\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Load and cache the bundles so we don't reload them every time.\n",
    "bundles = load_from_directory(spark, FHIR_JSON_BUNDLES).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Extract patient records into Spark Dataframes</li>\n",
    "    <li>Query and visualize patient records using Spark SQL </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>birthDate</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>male</td>\n",
       "      <td>1980-11-07</td>\n",
       "      <td>[Pittsfield]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:uuid:345efce8-d11d-429d-9984-6b67e31a7269</td>\n",
       "      <td>male</td>\n",
       "      <td>1952-06-04</td>\n",
       "      <td>[Harwich]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:uuid:44810270-bafe-42a4-8fc8-c229368b0058</td>\n",
       "      <td>male</td>\n",
       "      <td>1966-02-17</td>\n",
       "      <td>[Hubbardston]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urn:uuid:d6be5e17-7733-4096-b3a7-32c2a80582af</td>\n",
       "      <td>female</td>\n",
       "      <td>2018-12-29</td>\n",
       "      <td>[Worcester]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urn:uuid:5c6ad3ff-99b1-47b3-92c1-a37d82a5a559</td>\n",
       "      <td>male</td>\n",
       "      <td>1961-03-13</td>\n",
       "      <td>[Methuen Town]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>urn:uuid:e3952c11-3fa2-4492-899c-bbbb8c7b6db0</td>\n",
       "      <td>male</td>\n",
       "      <td>1956-07-01</td>\n",
       "      <td>[Wareham]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>urn:uuid:665b7d87-1e8a-46f5-a2fb-6e8200f6662e</td>\n",
       "      <td>male</td>\n",
       "      <td>1952-08-05</td>\n",
       "      <td>[Hudson]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>urn:uuid:08e56bf9-7034-4b6e-8345-c61a0d910c6e</td>\n",
       "      <td>female</td>\n",
       "      <td>1963-03-29</td>\n",
       "      <td>[Brockton]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>urn:uuid:e272d8a3-73c9-4887-a457-f0d1d7cc1e44</td>\n",
       "      <td>female</td>\n",
       "      <td>2003-11-19</td>\n",
       "      <td>[Weymouth Town]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>urn:uuid:1d9e528b-18b4-4cfa-bfd4-d2eb85e9ce1b</td>\n",
       "      <td>female</td>\n",
       "      <td>1984-11-14</td>\n",
       "      <td>[Lowell]</td>\n",
       "      <td>[Massachusetts]</td>\n",
       "      <td>[US]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              id  gender   birthDate  \\\n",
       "0  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354    male  1980-11-07   \n",
       "1  urn:uuid:345efce8-d11d-429d-9984-6b67e31a7269    male  1952-06-04   \n",
       "2  urn:uuid:44810270-bafe-42a4-8fc8-c229368b0058    male  1966-02-17   \n",
       "3  urn:uuid:d6be5e17-7733-4096-b3a7-32c2a80582af  female  2018-12-29   \n",
       "4  urn:uuid:5c6ad3ff-99b1-47b3-92c1-a37d82a5a559    male  1961-03-13   \n",
       "5  urn:uuid:e3952c11-3fa2-4492-899c-bbbb8c7b6db0    male  1956-07-01   \n",
       "6  urn:uuid:665b7d87-1e8a-46f5-a2fb-6e8200f6662e    male  1952-08-05   \n",
       "7  urn:uuid:08e56bf9-7034-4b6e-8345-c61a0d910c6e  female  1963-03-29   \n",
       "8  urn:uuid:e272d8a3-73c9-4887-a457-f0d1d7cc1e44  female  2003-11-19   \n",
       "9  urn:uuid:1d9e528b-18b4-4cfa-bfd4-d2eb85e9ce1b  female  1984-11-14   \n",
       "\n",
       "              city            state country age  \n",
       "0     [Pittsfield]  [Massachusetts]    [US]  38  \n",
       "1        [Harwich]  [Massachusetts]    [US]  66  \n",
       "2    [Hubbardston]  [Massachusetts]    [US]  52  \n",
       "3      [Worcester]  [Massachusetts]    [US]   0  \n",
       "4   [Methuen Town]  [Massachusetts]    [US]  57  \n",
       "5        [Wareham]  [Massachusetts]    [US]  62  \n",
       "6         [Hudson]  [Massachusetts]    [US]  66  \n",
       "7       [Brockton]  [Massachusetts]    [US]  55  \n",
       "8  [Weymouth Town]  [Massachusetts]    [US]  15  \n",
       "9         [Lowell]  [Massachusetts]    [US]  34  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract patients from bundles\n",
    "patients = extract_entry(spark, bundles, 'patient')\n",
    "\n",
    "pats = patients.select('id','gender', 'birthDate', 'address.city', 'address.state', 'address.country') \n",
    "\n",
    "#pats['birthDate'] = pats['birthDate'].apply(age)\n",
    "patsDF = pats.limit(10).toPandas()\n",
    "patsDF['age'] = patsDF['birthDate'].apply(age)\n",
    "display(patsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Extract Patient Encounters into Spark Dataframes</li>\n",
    "    <li>Query and visualize Encounter records using Spark SQL </li>\n",
    "    <li>Compute Length of Stay from Encounter start and end dates. </li>\n",
    "    <li>We will use Length of Stay and other features from Patient, Observation and other records to train our linear regression model.</li>\n",
    "    <li>Our linear regression model will predict label: \"Length of Stay\"</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>code</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1994-12-11T11:05:54-08:00</td>\n",
       "      <td>1994-12-12T11:20:54-08:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-04-06T12:05:54-07:00</td>\n",
       "      <td>1995-04-07T12:20:54-07:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-06-19T12:05:54-07:00</td>\n",
       "      <td>1995-06-20T12:05:54-07:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-08-25T12:05:54-07:00</td>\n",
       "      <td>1995-08-26T12:05:54-07:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1995-11-28T11:05:54-08:00</td>\n",
       "      <td>1995-11-29T11:05:54-08:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-01-18T11:05:54-08:00</td>\n",
       "      <td>1996-01-19T11:05:54-08:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-03-02T11:05:54-08:00</td>\n",
       "      <td>1996-03-03T11:20:54-08:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-04-22T12:05:54-07:00</td>\n",
       "      <td>1996-04-23T12:20:54-07:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-10-01T12:05:54-07:00</td>\n",
       "      <td>1996-10-02T12:05:54-07:00</td>\n",
       "      <td>1 day, 0:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>urn:uuid:c127185e-9f14-462a-9817-c90963fb7354</td>\n",
       "      <td>inpatient</td>\n",
       "      <td>1996-12-17T11:05:54-08:00</td>\n",
       "      <td>1996-12-18T11:20:54-08:00</td>\n",
       "      <td>1 day, 0:15:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       reference       code  \\\n",
       "0  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "1  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "2  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "3  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "4  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "5  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "6  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "7  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "8  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "9  urn:uuid:c127185e-9f14-462a-9817-c90963fb7354  inpatient   \n",
       "\n",
       "                       start                        end             los  \n",
       "0  1994-12-11T11:05:54-08:00  1994-12-12T11:20:54-08:00  1 day, 0:15:00  \n",
       "1  1995-04-06T12:05:54-07:00  1995-04-07T12:20:54-07:00  1 day, 0:15:00  \n",
       "2  1995-06-19T12:05:54-07:00  1995-06-20T12:05:54-07:00  1 day, 0:00:00  \n",
       "3  1995-08-25T12:05:54-07:00  1995-08-26T12:05:54-07:00  1 day, 0:00:00  \n",
       "4  1995-11-28T11:05:54-08:00  1995-11-29T11:05:54-08:00  1 day, 0:00:00  \n",
       "5  1996-01-18T11:05:54-08:00  1996-01-19T11:05:54-08:00  1 day, 0:00:00  \n",
       "6  1996-03-02T11:05:54-08:00  1996-03-03T11:20:54-08:00  1 day, 0:15:00  \n",
       "7  1996-04-22T12:05:54-07:00  1996-04-23T12:20:54-07:00  1 day, 0:15:00  \n",
       "8  1996-10-01T12:05:54-07:00  1996-10-02T12:05:54-07:00  1 day, 0:00:00  \n",
       "9  1996-12-17T11:05:54-08:00  1996-12-18T11:20:54-08:00  1 day, 0:15:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from demo_utils import los\n",
    "\n",
    "# Extract encounters from bundles\n",
    "encounters = extract_entry(spark, bundles, 'encounter') \n",
    "\n",
    "encs=encounters.select('subject.reference', \n",
    "                  'class.code', \n",
    "                  'period.start', \n",
    "                  'period.end') \\\n",
    "          .where(col('class.code').isin(\"inpatient\", \"emergency\"))\n",
    "\n",
    "\n",
    "encsDF = encs.limit(10).toPandas()\n",
    "encsDF['los'] = encsDF.apply(los, axis=1)\n",
    "display(encsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Label generation - Generate Labels in TFRecord format </h2>\n",
    "<ul>\n",
    "    <li>The next few cells generates labels from bundles in TFRecord format</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine GCS bucket that holds the bundels in TFRecord format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32501370  2019-01-08T23:10:08Z  gs://cluster-data/demo/data/bundles/bundles-00000-of-00010.tfrecords\n",
      "  39740597  2019-01-08T23:10:08Z  gs://cluster-data/demo/data/bundles/bundles-00001-of-00010.tfrecords\n",
      "  32894855  2019-01-08T23:10:09Z  gs://cluster-data/demo/data/bundles/bundles-00002-of-00010.tfrecords\n",
      "  30817812  2019-01-08T23:10:10Z  gs://cluster-data/demo/data/bundles/bundles-00003-of-00010.tfrecords\n",
      "  33319395  2019-01-08T23:10:11Z  gs://cluster-data/demo/data/bundles/bundles-00004-of-00010.tfrecords\n",
      "  48719477  2019-01-08T23:10:11Z  gs://cluster-data/demo/data/bundles/bundles-00005-of-00010.tfrecords\n",
      "  42681976  2019-01-08T23:10:12Z  gs://cluster-data/demo/data/bundles/bundles-00006-of-00010.tfrecords\n",
      "  32319546  2019-01-08T23:10:13Z  gs://cluster-data/demo/data/bundles/bundles-00007-of-00010.tfrecords\n",
      "  49995527  2019-01-08T23:10:14Z  gs://cluster-data/demo/data/bundles/bundles-00008-of-00010.tfrecords\n",
      "  36610205  2019-01-08T23:10:14Z  gs://cluster-data/demo/data/bundles/bundles-00009-of-00010.tfrecords\n",
      "TOTAL: 10 objects, 379600760 bytes (362.02 MiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l ${BUNDLES_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete labels generated from previous runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cluster-data/demo/data/output/labels-00000-of-00001.tfrecords...\n",
      "/ [1 objects]                                                                   \r\n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil rm ${LABELS_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/ipykernel/__main__.py:9: ImportWarning: Not importing directory '/usr/local/fhir/proto': missing __init__.py\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "import apache_beam as beam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "from proto.stu3 import google_extensions_pb2\n",
    "from proto.stu3 import resources_pb2\n",
    "from proto.stu3 import version_config_pb2\n",
    "\n",
    "from google.protobuf import text_format\n",
    "from py.google.fhir.labels import label\n",
    "from py.google.fhir.labels import bundle_to_label\n",
    "from py.google.fhir.seqex import bundle_to_seqex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set options needed to initialize the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = GCP_PROJECT\n",
    "google_cloud_options.job_name = LABELS_JOB\n",
    "google_cloud_options.staging_location = STAGING_LOCATION\n",
    "google_cloud_options.temp_location = TEMP_LOCATION\n",
    "options.view_as(StandardOptions).runner = RUNNER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the pipeline to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/oauth2client/contrib/gce.py:99: UserWarning: You have requested explicit scopes to be used with a GCE service account.\n",
      "Using this argument will have no effect on the actual scopes for tokens\n",
      "requested. These scopes are set at VM instance creation time and\n",
      "can't be overridden in the request.\n",
      "\n",
      "  warnings.warn(_SCOPES_WARNING)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "bundles = p | 'read' >> beam.io.ReadFromTFRecord(\n",
    "    TF_RECORD_BUNDLES, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "\n",
    "labels = bundles | 'BundleToLabel' >> beam.ParDo(\n",
    "    bundle_to_label.LengthOfStayRangeLabelAt24HoursFn(for_synthea=True))\n",
    "\n",
    "_ = labels | beam.io.WriteToTFRecord(\n",
    "    LABELS_PATH,\n",
    "    coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel),\n",
    "    file_name_suffix='.tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipeline to generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0125 01:17:19.932380 139864189060864 tfrecordio.py:49] Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the output location in GCS where labels have been crearted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     97285  2019-01-25T01:17:45Z  gs://cluster-data/demo/data/output/labels-00000-of-00001.tfrecords\n",
      "TOTAL: 1 objects, 97285 bytes (95 KiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l ${LABELS_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Generate TFSequenceExamples</h2>\n",
    "<ul>\n",
    "    <li>The next few cell generates Tensorflow sequence examples</li>\n",
    "    <li>Bundles in TFRecord format have already been generated from Synthetic FHIR data</li>\n",
    "    <li>Bundles will be used as inputs and are stored in Google Cloud Storage</li>\n",
    "    <li>Output labels will also be stored in Google Cloud Storage </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_seqex import _get_version_config \n",
    "\n",
    "google_cloud_options.job_name = SEQEX_JOB\n",
    "p1 = beam.Pipeline(options=options)\n",
    "\n",
    "version_config = _get_version_config(\"/usr/local/fhir/proto/stu3/version_config.textproto\")\n",
    "\n",
    "keyed_bundles = ( \n",
    "    p1 \n",
    "    | 'readBundles' >> beam.io.ReadFromTFRecord(\n",
    "        TF_RECORD_BUNDLES, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "    | 'KeyBundlesByPatientId' >> beam.ParDo(\n",
    "        bundle_to_seqex.KeyBundleByPatientIdFn()))\n",
    "\n",
    "event_labels = ( \n",
    "    p1 | 'readEventLabels' >> beam.io.ReadFromTFRecord(\n",
    "        TF_RECORD_LABELS,\n",
    "        coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel)))\n",
    "\n",
    "keyed_event_labels = bundle_to_seqex.CreateTriggerLabelsPairLists(\n",
    "    event_labels)\n",
    "\n",
    "bundles_and_labels = bundle_to_seqex.CreateBundleAndLabels(\n",
    "    keyed_bundles, keyed_event_labels)\n",
    "_ = ( \n",
    "    bundles_and_labels\n",
    "    | 'Reshuffle1' >> beam.Reshuffle()\n",
    "    | 'GenerateSeqex' >> beam.ParDo(\n",
    "        bundle_to_seqex.BundleAndLabelsToSeqexDoFn(\n",
    "            version_config=version_config,\n",
    "            enable_attribution=False,\n",
    "            generate_sequence_label=False))\n",
    "    | 'Reshuffle2' >> beam.Reshuffle()\n",
    "    | 'WriteSeqex' >> beam.io.WriteToTFRecord(\n",
    "        SEQEX_PATH,\n",
    "        coder=beam.coders.ProtoCoder(example_pb2.SequenceExample),\n",
    "        file_name_suffix='.tfrecords',\n",
    "        num_shards=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l ${SEQEX_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Train and Evaluate ML Model</h2>\n",
    "<ul>\n",
    "    <li>The next few cell demonstrate the process to train a ML Model using the training data set created in Step 3</li>\n",
    "    <li>Training requires sequence examples in TFRecord format</li>\n",
    "    <li>Trained ML model will be stored in Google Cloud Storage </li>\n",
    "    <li>Model will be evaluated and the evaluation output will be printed</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from training_utils import create_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_KEY_PREFIX = 'c-'\n",
    "SEQUENCE_KEY_PREFIX = 's-'\n",
    "AGE_KEY = 'Patient.ageInYears'\n",
    "LABEL_VALUES = ['less_or_equal_3', '3_7', '7_14', 'above_14']\n",
    "\n",
    "\n",
    "def _example_index_to_sparse_index(example_indices, batch_size):\n",
    "  \"\"\"Creates a sparse index tensor from a list of example indices.\n",
    "\n",
    "  For example, this would do the transformation:\n",
    "  [0, 0, 0, 1, 3, 3] -> [[0,0], [0,1], [0,2], [1,0], [3,0], [3,1]]\n",
    "\n",
    "  The second column of the output tensor is the running count of the occurrences\n",
    "  of that example index.\n",
    "\n",
    "  Args:\n",
    "    example_indices: A sorted 1D Tensor with example indices.\n",
    "    batch_size: The batch_size. Could be larger than max(example_indices) if the\n",
    "      last examples of the batch do not have the feature present.\n",
    "  Returns:\n",
    "    The sparse index tensor.\n",
    "    The maxmium length of a row in this tensor.\n",
    "  \"\"\"\n",
    "  binned_counts = tf.bincount(example_indices, minlength=batch_size)\n",
    "  max_len = tf.to_int64(tf.reduce_max(binned_counts))\n",
    "  return tf.where(tf.sequence_mask(binned_counts)), max_len\n",
    "\n",
    "def _dedup_tensor(sp_tensor):\n",
    "  \"\"\"Dedup values of a SparseTensor along each row.\n",
    "\n",
    "  Args:\n",
    "    sp_tensor: A 2D SparseTensor to be deduped.\n",
    "  Returns:\n",
    "    A deduped SparseTensor of shape [batch_size, max_len], where max_len is\n",
    "    the maximum number of unique values for a row in the Tensor.\n",
    "  \"\"\"\n",
    "  string_batch_index = tf.as_string(sp_tensor.indices[:, 0])\n",
    "\n",
    "  # tf.unique only works on 1D tensors. To avoid deduping across examples,\n",
    "  # prepend each feature value with the example index. This requires casting\n",
    "  # to and from strings for non-string features.\n",
    "  original_dtype = sp_tensor.values.dtype\n",
    "  string_values = (\n",
    "      sp_tensor.values\n",
    "      if original_dtype == tf.string else tf.as_string(sp_tensor.values))\n",
    "  index_and_value = tf.string_join([string_batch_index, string_values],\n",
    "                                   separator='|')\n",
    "  unique_index_and_value, _ = tf.unique(index_and_value)\n",
    "\n",
    "  # split is a shape [tf.size(values), 2] tensor. The first column contains\n",
    "  # indices and the second column contains the feature value (we assume no\n",
    "  # feature contains | so we get exactly 2 values from the string split).\n",
    "  split = tf.string_split(unique_index_and_value, delimiter='|')\n",
    "  split = tf.reshape(split.values, [-1, 2])\n",
    "  string_indices = split[:, 0]\n",
    "  values = split[:, 1]\n",
    "\n",
    "  indices = tf.reshape(\n",
    "      tf.string_to_number(string_indices, out_type=tf.int32), [-1])\n",
    "  if original_dtype != tf.string:\n",
    "    values = tf.string_to_number(values, out_type=original_dtype)\n",
    "  values = tf.reshape(values, [-1])\n",
    "  # Convert example indices into SparseTensor indices, e.g.\n",
    "  # [0, 0, 0, 1, 3, 3] -> [[0,0], [0,1], [0,2], [1,0], [3,0], [3,1]]\n",
    "  batch_size = tf.to_int32(sp_tensor.dense_shape[0])\n",
    "  new_indices, max_len = _example_index_to_sparse_index(indices, batch_size)\n",
    "  return tf.SparseTensor(\n",
    "      indices=tf.to_int64(new_indices),\n",
    "      values=values,\n",
    "      dense_shape=[tf.to_int64(batch_size), max_len])\n",
    "\n",
    "def get_input_fn(mode,\n",
    "                 input_pattern,\n",
    "                 dedup,\n",
    "                 time_windows,\n",
    "                 include_age,\n",
    "                 categorical_context_features,\n",
    "                 sequence_features,\n",
    "                 time_crossed_features,\n",
    "                 batch_size,\n",
    "                 shuffle=True):\n",
    "  \"\"\"Creates an input function to an estimator.\n",
    "\n",
    "  Args:\n",
    "    mode: The execution mode, as defined in tf.estimator.ModeKeys.\n",
    "    input_pattern: Input data pattern in TFRecord format containing\n",
    "      tf.SequenceExamples.\n",
    "    dedup: Whether to remove duplicate values.\n",
    "    time_windows: List of time windows - we bucket all sequence features by\n",
    "      their age into buckets [time_windows[i], time_windows[i+1]).\n",
    "    include_age: Whether to include the age_in_years as a feature.\n",
    "    categorical_context_features: List of string context features that are valid\n",
    "      keys in the tf.SequenceExample.\n",
    "    sequence_features: List of sequence features (strings) that are valid keys\n",
    "      in the tf.SequenceExample.\n",
    "    time_crossed_features: List of list of sequence features (strings) that\n",
    "      should be crossed at each step along the time dimension.\n",
    "    batch_size: The size of the batch when reading in data.\n",
    "    shuffle: Whether to shuffle the examples.\n",
    "\n",
    "  Returns:\n",
    "    A function that returns a dictionary of features and the target labels.\n",
    "  \"\"\"\n",
    "\n",
    "  def input_fn():\n",
    "    \"\"\"Supplies input to our model.\n",
    "\n",
    "    This function supplies input to our model, where this input is a\n",
    "    function of the mode. For example, we supply different data if\n",
    "    we're performing training versus evaluation.\n",
    "\n",
    "    Returns:\n",
    "      A tuple consisting of 1) a dictionary of tensors whose keys are\n",
    "      the feature names, and 2) a tensor of target labels if the mode\n",
    "      is not INFER (and None, otherwise).\n",
    "    \"\"\"\n",
    "\n",
    "    sequence_features_config = dict()\n",
    "    for feature in sequence_features:\n",
    "      dtype = tf.string\n",
    "      if feature == 'Observation.value.quantity.value':\n",
    "        dtype = tf.float32\n",
    "      sequence_features_config[feature] = tf.VarLenFeature(dtype)\n",
    "\n",
    "    sequence_features_config['eventId'] = tf.FixedLenSequenceFeature(\n",
    "        [], tf.int64, allow_missing=False)\n",
    "    for cross in time_crossed_features:\n",
    "      for feature in cross:\n",
    "        dtype = tf.string\n",
    "        if feature == 'Observation.value.quantity.value':\n",
    "          dtype = tf.float32\n",
    "        sequence_features_config[feature] = tf.VarLenFeature(dtype)\n",
    "    context_features_config = dict()\n",
    "    if include_age:\n",
    "      context_features_config['timestamp'] = tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=-1)\n",
    "      context_features_config['Patient.birthDate'] = tf.FixedLenFeature(\n",
    "          [], tf.int64, default_value=-1)\n",
    "    context_features_config['sequenceLength'] = tf.FixedLenFeature(\n",
    "        [], tf.int64, default_value=-1)\n",
    "\n",
    "    for context_feature in categorical_context_features:\n",
    "      context_features_config[context_feature] = tf.VarLenFeature(tf.string)\n",
    "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "      context_features_config['label.length_of_stay_range.class'] = (\n",
    "          tf.FixedLenFeature([], tf.string, default_value='MISSING'))\n",
    "\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    num_epochs = None if is_training else 1\n",
    "\n",
    "    with tf.name_scope('read_batch'):\n",
    "      file_names = [input_pattern]\n",
    "      files = tf.data.Dataset.list_files(file_names)\n",
    "      if shuffle:\n",
    "        files = files.shuffle(buffer_size=len(file_names))\n",
    "      dataset = (files\n",
    "                 .apply(tf.contrib.data.parallel_interleave(\n",
    "                     tf.data.TFRecordDataset, cycle_length=10))\n",
    "                 .repeat(num_epochs))\n",
    "      if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=100)\n",
    "      dataset = dataset.batch(batch_size)\n",
    "\n",
    "      def _parse_fn(serialized_examples):\n",
    "        context, sequence, _ = tf.io.parse_sequence_example(\n",
    "            serialized_examples,\n",
    "            context_features=context_features_config,\n",
    "            sequence_features=sequence_features_config,\n",
    "            name='parse_sequence_example')\n",
    "        return context, sequence\n",
    "\n",
    "      dataset = dataset.map(_parse_fn, num_parallel_calls=8)\n",
    "\n",
    "      def _process(context, sequence):\n",
    "        \"\"\"Supplies input to our model.\n",
    "\n",
    "        This function supplies input to our model after parsing.\n",
    "\n",
    "        Args:\n",
    "          context: The dictionary from key to (Sparse)Tensors with context\n",
    "            features\n",
    "          sequence: The dictionary from key to (Sparse)Tensors with sequence\n",
    "            features\n",
    "\n",
    "        Returns:\n",
    "          A tuple consisting of 1) a dictionary of tensors whose keys are\n",
    "          the feature names, and 2) a tensor of target labels if the mode\n",
    "          is not INFER (and None, otherwise).\n",
    "        \"\"\"\n",
    "        # Combine into a single dictionary.\n",
    "        feature_map = {}\n",
    "        # Add age if requested.\n",
    "        if include_age:\n",
    "          age_in_seconds = (\n",
    "              context['timestamp'] -\n",
    "              context.pop('Patient.birthDate'))\n",
    "          age_in_years = tf.to_float(age_in_seconds) / (60 * 60 * 24 * 365.0)\n",
    "          feature_map[CONTEXT_KEY_PREFIX + AGE_KEY] = age_in_years\n",
    "\n",
    "        sequence_length = context.pop('sequenceLength')\n",
    "        # Cross the requested features.\n",
    "        for cross in time_crossed_features:\n",
    "          # The features may be missing at different rates - we take the union\n",
    "          # of the indices supplying defaults.\n",
    "          extended_features = dict()\n",
    "          dense_shape = tf.concat(\n",
    "              [[tf.shape(sequence_length)[0]], [tf.reduce_max(sequence_length)],\n",
    "               tf.constant([1], dtype=tf.int64)],\n",
    "              axis=0)\n",
    "          for i, feature in enumerate(cross):\n",
    "            sp_tensor = sequence[feature]\n",
    "            additional_indices = []\n",
    "            covered_indices = sp_tensor.indices\n",
    "            for j, other_feature in enumerate(cross):\n",
    "              if i != j:\n",
    "                additional_indices.append(\n",
    "                    tf.sets.set_difference(\n",
    "                        tf.sparse_reorder(\n",
    "                            tf.SparseTensor(\n",
    "                                indices=sequence[other_feature].indices,\n",
    "                                values=tf.zeros([\n",
    "                                    tf.shape(sequence[other_feature].indices)[0]\n",
    "                                ],\n",
    "                                                dtype=tf.int32),\n",
    "                                dense_shape=dense_shape)),\n",
    "                        tf.sparse_reorder(\n",
    "                            tf.SparseTensor(\n",
    "                                indices=covered_indices,\n",
    "                                values=tf.zeros([tf.shape(covered_indices)[0]],\n",
    "                                                dtype=tf.int32),\n",
    "                                dense_shape=dense_shape))).indices)\n",
    "                covered_indices = tf.concat(\n",
    "                    [sp_tensor.indices] + additional_indices, axis=0)\n",
    "\n",
    "            additional_indices = tf.concat(additional_indices, axis=0)\n",
    "\n",
    "            # Supply defaults for all other indices.\n",
    "            default = tf.tile(\n",
    "                tf.constant(['n/a']),\n",
    "                multiples=[tf.shape(additional_indices)[0]])\n",
    "\n",
    "            string_value = (\n",
    "                tf.as_string(sp_tensor.values)\n",
    "                if sp_tensor.values.dtype != tf.string else sp_tensor.values)\n",
    "\n",
    "            extended_features[feature] = tf.sparse_reorder(\n",
    "                tf.SparseTensor(\n",
    "                    indices=tf.concat([sp_tensor.indices, additional_indices],\n",
    "                                      axis=0),\n",
    "                    values=tf.concat([string_value, default], axis=0),\n",
    "                    dense_shape=dense_shape))\n",
    "\n",
    "          new_values = tf.string_join(\n",
    "              [extended_features[f].values for f in cross], separator='-')\n",
    "          crossed_sp_tensor = tf.sparse_reorder(\n",
    "              tf.SparseTensor(\n",
    "                  indices=extended_features[cross[0]].indices,\n",
    "                  values=new_values,\n",
    "                  dense_shape=extended_features[cross[0]].dense_shape))\n",
    "          sequence['_'.join(cross)] = crossed_sp_tensor\n",
    "        # Remove unwanted features that are used in the cross but should not be\n",
    "        # considered outside the cross.\n",
    "        for cross in time_crossed_features:\n",
    "          for feature in cross:\n",
    "            if feature not in sequence_features and feature in sequence:\n",
    "              del sequence[feature]\n",
    "\n",
    "        # Flatten sparse tensor to compute event age. This dense tensor also\n",
    "        # contains padded values. These will not be used when gathering elements\n",
    "        # from the dense tensor since each sparse feature won't have a value\n",
    "        # defined for the padding.\n",
    "        padded_event_age = (\n",
    "            # Broadcast current time along sequence dimension.\n",
    "            tf.expand_dims(context.pop('timestamp'), 1)\n",
    "            # Subtract time of events.\n",
    "            - sequence.pop('eventId'))\n",
    "\n",
    "        for i in range(len(time_windows) - 1):\n",
    "          max_age = time_windows[i]\n",
    "          min_age = time_windows[i+1]\n",
    "          padded_in_time_window = tf.logical_and(padded_event_age <= max_age,\n",
    "                                                 padded_event_age > min_age)\n",
    "\n",
    "          for k, v in sequence.items():\n",
    "            # For each sparse feature entry, look up whether it is in the time\n",
    "            # window or not.\n",
    "            in_time_window = tf.gather_nd(padded_in_time_window,\n",
    "                                          v.indices[:, 0:2])\n",
    "            v = tf.sparse_retain(v, in_time_window)\n",
    "            sp_tensor = tf.sparse_reshape(v, [v.dense_shape[0], -1])\n",
    "            if dedup:\n",
    "              sp_tensor = _dedup_tensor(sp_tensor)\n",
    "\n",
    "            feature_map[SEQUENCE_KEY_PREFIX + k +\n",
    "                        '-til-%d' %min_age] = sp_tensor\n",
    "\n",
    "        for k, v in context.items():\n",
    "          feature_map[CONTEXT_KEY_PREFIX + k] = v\n",
    "        return feature_map\n",
    "\n",
    "      feature_map = (dataset\n",
    "                     # Parallelize the input processing and put it behind a\n",
    "                     # queue to increase performance by removing it from the\n",
    "                     # critical path of per-step-computation.\n",
    "                     .map(_process, num_parallel_calls=8)\n",
    "                     .prefetch(buffer_size=1)\n",
    "                     .make_one_shot_iterator()\n",
    "                     .get_next())\n",
    "      label = None\n",
    "      if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "        label = feature_map.pop(CONTEXT_KEY_PREFIX +\n",
    "                                'label.length_of_stay_range.class')\n",
    "      return feature_map, label\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-0ec1adfd2a79>:158: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0125 01:33:38.248111 140371859998464 tf_logging.py:125] From <ipython-input-8-0ec1adfd2a79>:158: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.parallel_interleave(...)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:33:41.466789 140371859998464 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:33:41.655812 140371859998464 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:33:41.734312 140371859998464 tf_logging.py:115] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s-Observation.code_Observation.value.quantity.value_Observation.value.quantity.unit_Observation.value.string-til-0': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.section.text.div.tokenized-til-2592000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Observation.code_Observation.value.quantity.value_Observation.value.quantity.unit_Observation.value.string-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.section.text.div.tokenized-til-0': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Condition.code-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.reason.hcc-til-2592000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Observation.code_Observation.value.quantity.value_Observation.value.quantity.unit_Observation.value.string-til-2592000': SparseTensorValue(indices=array([[ 1,  0],\n",
      "       [ 1,  1],\n",
      "       [ 1,  2],\n",
      "       [ 1,  3],\n",
      "       [ 1,  4],\n",
      "       [ 1,  5],\n",
      "       [ 1,  6],\n",
      "       [ 1,  7],\n",
      "       [ 1,  8],\n",
      "       [ 1,  9],\n",
      "       [ 1, 10],\n",
      "       [ 1, 11],\n",
      "       [ 1, 12],\n",
      "       [ 1, 13],\n",
      "       [ 1, 14],\n",
      "       [ 1, 15],\n",
      "       [ 1, 16],\n",
      "       [ 1, 17],\n",
      "       [ 1, 18],\n",
      "       [ 1, 19],\n",
      "       [ 1, 20],\n",
      "       [ 1, 21],\n",
      "       [ 1, 22],\n",
      "       [ 1, 23],\n",
      "       [ 1, 24],\n",
      "       [ 1, 25],\n",
      "       [ 1, 26],\n",
      "       [ 1, 27]]), values=array(['loinc:32465-7-n/a-n/a-n/a', 'loinc:2857-1-2.938404-ng/mL-n/a',\n",
      "       'loinc:39156-5-30.470497-kg/m2-n/a', 'loinc:72166-2-n/a-n/a-n/a',\n",
      "       'loinc:29463-7-93.057587-kg-n/a', 'loinc:8302-2-174.757614-cm-n/a',\n",
      "       'loinc:55284-4-n/a-n/a-n/a', 'loinc:72514-3-1.023154-{score}-n/a',\n",
      "       'loinc:1751-7-3.642727-g/dL-n/a',\n",
      "       'loinc:6298-4-3.862539-mmol/L-n/a',\n",
      "       'loinc:6768-6-93.831886-U/L-n/a',\n",
      "       'loinc:33914-3-73.759880-mL/min-n/a',\n",
      "       'loinc:18262-6-118.143150-mg/dL-n/a',\n",
      "       'loinc:2093-3-156.967529-mg/dL-n/a',\n",
      "       'loinc:2085-9-61.112003-mg/dL-n/a',\n",
      "       'loinc:1975-2-0.399675-mg/dL-n/a',\n",
      "       'loinc:20565-8-27.564201-mmol/L-n/a',\n",
      "       'loinc:2885-2-73.280090-g/dL-n/a',\n",
      "       'loinc:10834-0-2.421917-g/L-n/a',\n",
      "       'loinc:2069-3-109.777779-mmol/L-n/a',\n",
      "       'loinc:2339-0-97.561035-mg/dL-n/a',\n",
      "       'loinc:2947-0-140.382751-mmol/L-n/a',\n",
      "       'loinc:6299-2-17.684156-mg/dL-n/a',\n",
      "       'loinc:2571-8-106.169861-mg/dL-n/a',\n",
      "       'loinc:1920-8-6.251385-U/L-n/a',\n",
      "       'loinc:38483-4-2.834112-mg/dL-n/a',\n",
      "       'loinc:1742-6-53.506790-U/L-n/a',\n",
      "       'loinc:49765-1-9.326193-mg/dL-n/a'], dtype=object), dense_shape=array([ 2, 28])), 's-Encounter.hospitalization.admitSource-til-0': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Procedure.code.cpt-til-31536000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 'c-Patient.ageInYears': array([26.005573, 75.63762 ], dtype=float32), 's-MedicationRequest.contained.medication.code.gsn-til-2592000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.reason.hcc-til-0': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.reason.hcc-til-31536000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 'label': array(['3_7', 'less_or_equal_3'], dtype=object), 's-MedicationRequest.contained.medication.code.gsn-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Observation.code_Observation.value.quantity.value_Observation.value.quantity.unit_Observation.value.string-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Condition.code-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.reason.hcc-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-MedicationRequest.contained.medication.code.gsn-til-0': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.type-til-31536000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Condition.code-til-2592000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.section.text.div.tokenized-til-31536000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Procedure.code.cpt-til-0': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.type-til-2592000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.hospitalization.admitSource-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-MedicationRequest.contained.medication.code.gsn-til-31536000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Condition.code-til-0': SparseTensorValue(indices=array([[0, 0],\n",
      "       [0, 1]]), values=array(['snomed:428251008', 'snomed:74400008'], dtype=object), dense_shape=array([2, 2])), 's-Composition.type-til-0': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.section.text.div.tokenized-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Observation.code_Observation.value.quantity.value_Observation.value.quantity.unit_Observation.value.string-til-31536000': SparseTensorValue(indices=array([[  1,   0],\n",
      "       [  1,   1],\n",
      "       [  1,   2],\n",
      "       [  1,   3],\n",
      "       [  1,   4],\n",
      "       [  1,   5],\n",
      "       [  1,   6],\n",
      "       [  1,   7],\n",
      "       [  1,   8],\n",
      "       [  1,   9],\n",
      "       [  1,  10],\n",
      "       [  1,  11],\n",
      "       [  1,  12],\n",
      "       [  1,  13],\n",
      "       [  1,  14],\n",
      "       [  1,  15],\n",
      "       [  1,  16],\n",
      "       [  1,  17],\n",
      "       [  1,  18],\n",
      "       [  1,  19],\n",
      "       [  1,  20],\n",
      "       [  1,  21],\n",
      "       [  1,  22],\n",
      "       [  1,  23],\n",
      "       [  1,  24],\n",
      "       [  1,  25],\n",
      "       [  1,  26],\n",
      "       [  1,  27],\n",
      "       [  1,  28],\n",
      "       [  1,  29],\n",
      "       [  1,  30],\n",
      "       [  1,  31],\n",
      "       [  1,  32],\n",
      "       [  1,  33],\n",
      "       [  1,  34],\n",
      "       [  1,  35],\n",
      "       [  1,  36],\n",
      "       [  1,  37],\n",
      "       [  1,  38],\n",
      "       [  1,  39],\n",
      "       [  1,  40],\n",
      "       [  1,  41],\n",
      "       [  1,  42],\n",
      "       [  1,  43],\n",
      "       [  1,  44],\n",
      "       [  1,  45],\n",
      "       [  1,  46],\n",
      "       [  1,  47],\n",
      "       [  1,  48],\n",
      "       [  1,  49],\n",
      "       [  1,  50],\n",
      "       [  1,  51],\n",
      "       [  1,  52],\n",
      "       [  1,  53],\n",
      "       [  1,  54],\n",
      "       [  1,  55],\n",
      "       [  1,  56],\n",
      "       [  1,  57],\n",
      "       [  1,  58],\n",
      "       [  1,  59],\n",
      "       [  1,  60],\n",
      "       [  1,  61],\n",
      "       [  1,  62],\n",
      "       [  1,  63],\n",
      "       [  1,  64],\n",
      "       [  1,  65],\n",
      "       [  1,  66],\n",
      "       [  1,  67],\n",
      "       [  1,  68],\n",
      "       [  1,  69],\n",
      "       [  1,  70],\n",
      "       [  1,  71],\n",
      "       [  1,  72],\n",
      "       [  1,  73],\n",
      "       [  1,  74],\n",
      "       [  1,  75],\n",
      "       [  1,  76],\n",
      "       [  1,  77],\n",
      "       [  1,  78],\n",
      "       [  1,  79],\n",
      "       [  1,  80],\n",
      "       [  1,  81],\n",
      "       [  1,  82],\n",
      "       [  1,  83],\n",
      "       [  1,  84],\n",
      "       [  1,  85],\n",
      "       [  1,  86],\n",
      "       [  1,  87],\n",
      "       [  1,  88],\n",
      "       [  1,  89],\n",
      "       [  1,  90],\n",
      "       [  1,  91],\n",
      "       [  1,  92],\n",
      "       [  1,  93],\n",
      "       [  1,  94],\n",
      "       [  1,  95],\n",
      "       [  1,  96],\n",
      "       [  1,  97],\n",
      "       [  1,  98],\n",
      "       [  1,  99],\n",
      "       [  1, 100],\n",
      "       [  1, 101],\n",
      "       [  1, 102],\n",
      "       [  1, 103],\n",
      "       [  1, 104],\n",
      "       [  1, 105],\n",
      "       [  1, 106],\n",
      "       [  1, 107],\n",
      "       [  1, 108],\n",
      "       [  1, 109],\n",
      "       [  1, 110],\n",
      "       [  1, 111],\n",
      "       [  1, 112],\n",
      "       [  1, 113],\n",
      "       [  1, 114]]), values=array(['loinc:2857-1-3.236133-ng/mL-n/a', 'loinc:32465-7-n/a-n/a-n/a',\n",
      "       'loinc:39156-5-32.476223-kg/m2-n/a',\n",
      "       'loinc:29463-7-99.183113-kg-n/a', 'loinc:72166-2-n/a-n/a-n/a',\n",
      "       'loinc:72514-3-0.358654-{score}-n/a',\n",
      "       'loinc:8302-2-174.757614-cm-n/a', 'loinc:55284-4-n/a-n/a-n/a',\n",
      "       'loinc:6299-2-13.909937-mg/dL-n/a',\n",
      "       'loinc:33914-3-88.777382-mL/min-n/a',\n",
      "       'loinc:1920-8-11.695556-U/L-n/a',\n",
      "       'loinc:49765-1-10.113814-mg/dL-n/a',\n",
      "       'loinc:10834-0-2.319530-g/L-n/a',\n",
      "       'loinc:1975-2-0.544697-mg/dL-n/a',\n",
      "       'loinc:1742-6-36.370472-U/L-n/a', 'loinc:6768-6-67.063919-U/L-n/a',\n",
      "       'loinc:2093-3-186.636459-mg/dL-n/a',\n",
      "       'loinc:2069-3-104.295670-mmol/L-n/a',\n",
      "       'loinc:2571-8-113.883659-mg/dL-n/a',\n",
      "       'loinc:2085-9-76.684731-mg/dL-n/a',\n",
      "       'loinc:1751-7-3.611294-g/dL-n/a',\n",
      "       'loinc:2339-0-94.436913-mg/dL-n/a',\n",
      "       'loinc:2885-2-60.790802-g/dL-n/a',\n",
      "       'loinc:38483-4-2.921961-mg/dL-n/a',\n",
      "       'loinc:20565-8-22.259756-mmol/L-n/a',\n",
      "       'loinc:6298-4-3.958885-mmol/L-n/a',\n",
      "       'loinc:2947-0-141.138977-mmol/L-n/a',\n",
      "       'loinc:18262-6-94.886497-mg/dL-n/a',\n",
      "       'loinc:2857-1-2.466203-ng/mL-n/a',\n",
      "       'loinc:72514-3-3.620427-{score}-n/a',\n",
      "       'loinc:29463-7-97.862251-kg-n/a',\n",
      "       'loinc:39156-5-32.043720-kg/m2-n/a',\n",
      "       'loinc:38483-4-2.630746-mg/dL-n/a',\n",
      "       'loinc:49765-1-8.601586-mg/dL-n/a',\n",
      "       'loinc:1975-2-0.223621-mg/dL-n/a',\n",
      "       'loinc:33914-3-85.365974-mL/min-n/a',\n",
      "       'loinc:2571-8-131.078140-mg/dL-n/a',\n",
      "       'loinc:2085-9-61.495472-mg/dL-n/a',\n",
      "       'loinc:1751-7-4.050027-g/dL-n/a',\n",
      "       'loinc:2339-0-93.784065-mg/dL-n/a',\n",
      "       'loinc:6298-4-4.714720-mmol/L-n/a',\n",
      "       'loinc:20565-8-22.495777-mmol/L-n/a',\n",
      "       'loinc:6299-2-14.045341-mg/dL-n/a',\n",
      "       'loinc:10834-0-2.933535-g/L-n/a', 'loinc:1742-6-45.628391-U/L-n/a',\n",
      "       'loinc:2885-2-68.256203-g/dL-n/a',\n",
      "       'loinc:6768-6-118.798851-U/L-n/a',\n",
      "       'loinc:2947-0-139.004333-mmol/L-n/a',\n",
      "       'loinc:2069-3-110.331398-mmol/L-n/a',\n",
      "       'loinc:2093-3-191.677322-mg/dL-n/a',\n",
      "       'loinc:18262-6-107.034210-mg/dL-n/a',\n",
      "       'loinc:1920-8-33.551643-U/L-n/a',\n",
      "       'loinc:2857-1-1.896251-ng/mL-n/a',\n",
      "       'loinc:72514-3-3.551927-{score}-n/a',\n",
      "       'loinc:785-6-32.075439-pg-n/a',\n",
      "       'loinc:39156-5-31.415457-kg/m2-n/a',\n",
      "       'loinc:21000-5-40.238369-fL-n/a',\n",
      "       'loinc:777-3-182.999603-10*3/uL-n/a',\n",
      "       'loinc:789-8-4.266813-10*6/uL-n/a',\n",
      "       'loinc:2093-3-167.277084-mg/dL-n/a',\n",
      "       'loinc:786-4-34.888832-g/dL-n/a', 'loinc:718-7-16.614565-g/dL-n/a',\n",
      "       'loinc:29463-7-95.943512-kg-n/a', 'loinc:787-2-82.485062-fL-n/a',\n",
      "       'loinc:32207-3-479.620605-fL-n/a',\n",
      "       'loinc:18262-6-68.613274-mg/dL-n/a',\n",
      "       'loinc:32623-1-11.710562-fL-n/a', 'loinc:4544-3-47.332581-%-n/a',\n",
      "       'loinc:2085-9-77.944290-mg/dL-n/a',\n",
      "       'loinc:2571-8-103.597595-mg/dL-n/a',\n",
      "       'loinc:6690-2-7.243675-10*3/uL-n/a',\n",
      "       'loinc:1920-8-34.289219-U/L-n/a',\n",
      "       'loinc:2885-2-64.185196-g/dL-n/a',\n",
      "       'loinc:2085-9-49.789909-mg/dL-n/a',\n",
      "       'loinc:2093-3-157.073547-mg/dL-n/a',\n",
      "       'loinc:6768-6-63.292206-U/L-n/a',\n",
      "       'loinc:33914-3-88.265175-mL/min-n/a',\n",
      "       'loinc:10834-0-3.329831-g/L-n/a',\n",
      "       'loinc:1975-2-1.032518-mg/dL-n/a',\n",
      "       'loinc:2571-8-132.376541-mg/dL-n/a',\n",
      "       'loinc:2947-0-143.126190-mmol/L-n/a',\n",
      "       'loinc:49765-1-8.699318-mg/dL-n/a',\n",
      "       'loinc:1751-7-4.564139-g/dL-n/a',\n",
      "       'loinc:20565-8-27.820478-mmol/L-n/a',\n",
      "       'loinc:6298-4-3.820590-mmol/L-n/a',\n",
      "       'loinc:38483-4-3.186714-mg/dL-n/a',\n",
      "       'loinc:2339-0-87.542542-mg/dL-n/a',\n",
      "       'loinc:18262-6-116.376793-mg/dL-n/a',\n",
      "       'loinc:6299-2-14.227048-mg/dL-n/a',\n",
      "       'loinc:1742-6-20.002121-U/L-n/a',\n",
      "       'loinc:2069-3-104.568718-mmol/L-n/a',\n",
      "       'loinc:39156-5-30.809357-kg/m2-n/a',\n",
      "       'loinc:72514-3-2.942750-{score}-n/a',\n",
      "       'loinc:29463-7-94.092468-kg-n/a',\n",
      "       'loinc:2857-1-1.686958-ng/mL-n/a',\n",
      "       'loinc:18262-6-121.708794-mg/dL-n/a',\n",
      "       'loinc:1975-2-0.659925-mg/dL-n/a',\n",
      "       'loinc:10834-0-3.026513-g/L-n/a',\n",
      "       'loinc:6299-2-11.615704-mg/dL-n/a',\n",
      "       'loinc:49765-1-9.701925-mg/dL-n/a',\n",
      "       'loinc:6768-6-94.416306-U/L-n/a',\n",
      "       'loinc:20565-8-22.166985-mmol/L-n/a',\n",
      "       'loinc:6298-4-3.865185-mmol/L-n/a',\n",
      "       'loinc:2069-3-102.760567-mmol/L-n/a',\n",
      "       'loinc:1920-8-32.944931-U/L-n/a',\n",
      "       'loinc:38483-4-3.430364-mg/dL-n/a',\n",
      "       'loinc:1742-6-25.033598-U/L-n/a',\n",
      "       'loinc:2947-0-141.832092-mmol/L-n/a',\n",
      "       'loinc:2085-9-38.132069-mg/dL-n/a',\n",
      "       'loinc:2339-0-88.209297-mg/dL-n/a',\n",
      "       'loinc:2093-3-181.979996-mg/dL-n/a',\n",
      "       'loinc:2885-2-72.940598-g/dL-n/a',\n",
      "       'loinc:33914-3-74.845116-mL/min-n/a',\n",
      "       'loinc:2571-8-151.255020-mg/dL-n/a',\n",
      "       'loinc:1751-7-4.980924-g/dL-n/a'], dtype=object), dense_shape=array([  2, 115])), 's-Condition.code-til-31536000': SparseTensorValue(indices=array([[1, 0],\n",
      "       [1, 1],\n",
      "       [1, 2]]), values=array(['snomed:162573006', 'snomed:254632001', 'snomed:67811000119102'],\n",
      "      dtype=object), dense_shape=array([2, 3])), 's-Encounter.reason.hcc-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.hospitalization.admitSource-til-2592000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Procedure.code.cpt-til-2592000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.hospitalization.admitSource-til-31536000': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.type-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-MedicationRequest.contained.medication.code.gsn-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Procedure.code.cpt-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Procedure.code.cpt-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 'c-Patient.gender': SparseTensorValue(indices=array([[0, 0],\n",
      "       [1, 0]]), values=array(['male', 'male'], dtype=object), dense_shape=array([2, 1])), 's-Composition.section.text.div.tokenized-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Encounter.hospitalization.admitSource-til-86400': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0])), 's-Composition.type-til-604800': SparseTensorValue(indices=array([], shape=(0, 2), dtype=int64), values=array([], dtype=object), dense_shape=array([2, 0]))}\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "hparams = create_hparams()\n",
    "\n",
    "time_crossed_features = [\n",
    "        cross.split(':') for cross in hparams.time_crossed_features if cross\n",
    "    ]\n",
    "\n",
    "map_, label_ = get_input_fn(tf.estimator.ModeKeys.TRAIN, TRAINING_DATASET, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=2)()\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  map_['label'] = label_\n",
    "  print(sess.run(map_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:35:08.329107 140371859998464 tf_logging.py:115] Using default config.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa84f2fa10>, '_model_dir': 'gs://cluster-data/demo/data/output/model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:35:08.334386 140371859998464 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa84f2fa10>, '_model_dir': 'gs://cluster-data/demo/data/output/model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "seq_features = []\n",
    "seq_features_sizes = []\n",
    "hparams = create_hparams()\n",
    "\n",
    "for k, bucket_size in zip(\n",
    "    hparams.sequence_features,\n",
    "    hparams.sequence_bucket_sizes):\n",
    "  for max_age in hparams.time_windows[1:]:\n",
    "    seq_features.append(\n",
    "        tf.feature_column.categorical_column_with_hash_bucket(\n",
    "            SEQUENCE_KEY_PREFIX + k + '-til-' +\n",
    "            str(max_age), bucket_size))\n",
    "    seq_features_sizes.append(bucket_size)\n",
    "\n",
    "categorical_context_features = [\n",
    "    tf.feature_column.categorical_column_with_hash_bucket(\n",
    "        CONTEXT_KEY_PREFIX + k, bucket_size)\n",
    "    for k, bucket_size in zip(hparams.categorical_context_features,\n",
    "                              hparams.context_bucket_sizes)\n",
    "]\n",
    "discretized_context_features = []\n",
    "if hparams.include_age:\n",
    "  discretized_context_features.append(\n",
    "      tf.feature_column.bucketized_column(\n",
    "          tf.feature_column.numeric_column(CONTEXT_KEY_PREFIX + AGE_KEY),\n",
    "          boundaries=hparams.age_boundaries))\n",
    "\n",
    "optimizer = tf.train.FtrlOptimizer(\n",
    "      learning_rate=hparams.learning_rate,\n",
    "      l1_regularization_strength=hparams.l1_regularization_strength,\n",
    "      l2_regularization_strength=hparams.l2_regularization_strength)\n",
    "\n",
    "estimator = tf.estimator.LinearClassifier(\n",
    "    feature_columns=seq_features + categorical_context_features +\n",
    "    discretized_context_features,\n",
    "    n_classes=len(LABEL_VALUES),\n",
    "    label_vocabulary=LABEL_VALUES,\n",
    "    model_dir=MODEL_PATH,\n",
    "    optimizer=optimizer,\n",
    "    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa9600c250>, '_model_dir': 'gs://cluster-data/demo/data/output/model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:35:20.633589 140371859998464 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa9600c250>, '_model_dir': 'gs://cluster-data/demo/data/output/model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "def multiclass_metrics_fn(labels, predictions):\n",
    "  \"\"\"Computes precsion/recall@k metrics for each class and micro-weighted.\n",
    "\n",
    "  Args:\n",
    "    labels: A string Tensor of shape [batch_size] with the true labels\n",
    "    predictions: A float Tensor of shape [batch_size, num_classes].\n",
    "\n",
    "  Returns:\n",
    "    A dictionary with metrics of precision/recall @1/2 and precision/recall per\n",
    "    class.\n",
    "  \"\"\"\n",
    "\n",
    "  label_ids = tf.contrib.lookup.index_table_from_tensor(\n",
    "      tuple(LABEL_VALUES),\n",
    "      name='class_id_lookup').lookup(labels)\n",
    "  dense_labels = tf.one_hot(label_ids, len(LABEL_VALUES))\n",
    "\n",
    "  # We convert the task to a binary one of < 7 days.\n",
    "  # 'less_or_equal_3', '3_7', '7_14', 'above_14'\n",
    "  binary_labels = label_ids < 2\n",
    "  binary_probs = tf.reduce_sum(predictions['probabilities'][:, 0:2], axis=1)\n",
    "\n",
    "  metrics_dict = {\n",
    "      'precision_at_1':\n",
    "          tf.metrics.precision_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=1),\n",
    "      'precision_at_2':\n",
    "          tf.metrics.precision_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=2),\n",
    "      'recall_at_1':\n",
    "          tf.metrics.recall_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=1),\n",
    "      'recall_at_2':\n",
    "          tf.metrics.recall_at_k(\n",
    "              labels=label_ids,\n",
    "              predictions=predictions['probabilities'], k=2),\n",
    "      'auc_roc_at_most_7d':\n",
    "          tf.metrics.auc(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs,\n",
    "              curve='ROC',\n",
    "              summation_method='careful_interpolation'),\n",
    "      'auc_pr_at_most_7d':\n",
    "          tf.metrics.auc(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs,\n",
    "              curve='PR',\n",
    "              summation_method='careful_interpolation'),\n",
    "      'precision_at_most_7d':\n",
    "          tf.metrics.precision(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs >= 0.5),\n",
    "      'recall_at_most_7d':\n",
    "          tf.metrics.recall(\n",
    "              labels=binary_labels,\n",
    "              predictions=binary_probs >= 0.5),\n",
    "  }\n",
    "  for i, label in enumerate(LABEL_VALUES):\n",
    "    metrics_dict['precision_%s' % label] = tf.metrics.precision_at_k(\n",
    "        labels=label_ids,\n",
    "        predictions=predictions['probabilities'],\n",
    "        k=1,\n",
    "        class_id=i)\n",
    "    metrics_dict['recall_%s' % label] = tf.metrics.recall_at_k(\n",
    "        labels=label_ids,\n",
    "        predictions=predictions['probabilities'],\n",
    "        k=1,\n",
    "        class_id=i)\n",
    "\n",
    "  return metrics_dict\n",
    "estimator = tf.contrib.estimator.add_metrics(estimator, multiclass_metrics_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(tf.estimator.ModeKeys.TRAIN, TRAINING_DATASET, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:02.967899 140371859998464 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:02.971937 140371859998464 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:07.768445 140371859998464 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:07.773546 140371859998464 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:07.777867 140371859998464 tf_logging.py:115] Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:10.990098 140371859998464 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://cluster-data/demo/data/output/model/model.ckpt-400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:11.263156 140371859998464 tf_logging.py:115] Restoring parameters from gs://cluster-data/demo/data/output/model/model.ckpt-400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:13.016444 140371859998464 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:13.158279 140371859998464 tf_logging.py:115] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 400 into gs://cluster-data/demo/data/output/model/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:19.555378 140371859998464 tf_logging.py:115] Saving checkpoints for 400 into gs://cluster-data/demo/data/output/model/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.6670739, step = 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:32.294358 140371859998464 tf_logging.py:115] loss = 0.6670739, step = 401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 500 into gs://cluster-data/demo/data/output/model/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:40.884998 140371859998464 tf_logging.py:115] Saving checkpoints for 500 into gs://cluster-data/demo/data/output/model/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.61383367.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:36:51.219177 140371859998464 tf_logging.py:115] Loss for final step: 0.61383367.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7faa96081dd0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.train(input_fn=train_input_fn, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2019-01-25T01:36:44Z  gs://cluster-data/demo/data/output/model/\n",
      "       265  2019-01-25T01:36:47Z  gs://cluster-data/demo/data/output/model/checkpoint\n",
      "   9706057  2019-01-23T20:02:55Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548273732.cluster-002-m\n",
      "   9706130  2019-01-23T23:09:07Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548284902.cluster-002-m\n",
      "   9706133  2019-01-24T22:49:19Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548370118.cluster-002-m\n",
      "   9706133  2019-01-24T23:54:57Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548374056.cluster-001-m\n",
      "   9706133  2019-01-25T01:36:50Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548380168.cluster-002-m\n",
      "   5790532  2019-01-25T01:36:17Z  gs://cluster-data/demo/data/output/model/graph.pbtxt\n",
      "         8  2019-01-23T23:08:39Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.data-00000-of-00002\n",
      "   5393904  2019-01-23T23:08:38Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.data-00001-of-00002\n",
      "      5424  2019-01-23T23:08:40Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.index\n",
      "   2603529  2019-01-23T23:08:43Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.meta\n",
      "         8  2019-01-24T22:48:53Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.data-00000-of-00002\n",
      "   5393904  2019-01-24T22:48:52Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.data-00001-of-00002\n",
      "      5424  2019-01-24T22:48:53Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.index\n",
      "   2603529  2019-01-24T22:48:57Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.meta\n",
      "         8  2019-01-24T23:54:31Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.data-00000-of-00002\n",
      "   5393904  2019-01-24T23:54:31Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.data-00001-of-00002\n",
      "      5424  2019-01-24T23:54:32Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.index\n",
      "   2603529  2019-01-24T23:54:35Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.meta\n",
      "         8  2019-01-25T01:36:23Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.data-00000-of-00002\n",
      "   5393904  2019-01-25T01:36:22Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.data-00001-of-00002\n",
      "      5424  2019-01-25T01:36:23Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.index\n",
      "   2603529  2019-01-25T01:36:27Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.meta\n",
      "         8  2019-01-25T01:36:45Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.data-00000-of-00002\n",
      "   5393904  2019-01-25T01:36:45Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.data-00001-of-00002\n",
      "      5424  2019-01-25T01:36:46Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.index\n",
      "   2603529  2019-01-25T01:36:50Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.meta\n",
      "                                 gs://cluster-data/demo/data/output/model/eval/\n",
      "TOTAL: 28 objects, 94335708 bytes (89.97 MiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l ${MODEL_IN_GCS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input_fn = get_input_fn(tf.estimator.ModeKeys.EVAL, VALIDATION_DATASET, True, hparams.time_windows,\n",
    "                            hparams.include_age, hparams.categorical_context_features,\n",
    "                            hparams.sequence_features, time_crossed_features, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:48.092363 140371859998464 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:48.100545 140371859998464 tf_logging.py:115] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:50.802532 140371859998464 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:52.363790 140371859998464 tf_logging.py:115] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2019-01-25-01:39:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:52.393117 140371859998464 tf_logging.py:115] Starting evaluation at 2019-01-25-01:39:52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:52.773422 140371859998464 tf_logging.py:115] Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from gs://cluster-data/demo/data/output/model/model.ckpt-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:52.900044 140371859998464 tf_logging.py:115] Restoring parameters from gs://cluster-data/demo/data/output/model/model.ckpt-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:54.142343 140371859998464 tf_logging.py:115] Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:54.291296 140371859998464 tf_logging.py:115] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2019-01-25-01:39:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:58.895245 140371859998464 tf_logging.py:115] Finished evaluation at 2019-01-25-01:39:58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.5, auc_pr_at_most_7d = 1.0, auc_roc_at_most_7d = 0.0, average_loss = 1.2013555, global_step = 500, loss = 1.2013555, precision_3_7 = nan, precision_7_14 = nan, precision_above_14 = nan, precision_at_1 = 0.5, precision_at_2 = 0.5, precision_at_most_7d = 1.0, precision_less_or_equal_3 = 0.5, recall_3_7 = 0.0, recall_7_14 = nan, recall_above_14 = nan, recall_at_1 = 0.5, recall_at_2 = 1.0, recall_at_most_7d = 1.0, recall_less_or_equal_3 = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:39:58.900135 140371859998464 tf_logging.py:115] Saving dict for global step 500: accuracy = 0.5, auc_pr_at_most_7d = 1.0, auc_roc_at_most_7d = 0.0, average_loss = 1.2013555, global_step = 500, loss = 1.2013555, precision_3_7 = nan, precision_7_14 = nan, precision_above_14 = nan, precision_at_1 = 0.5, precision_at_2 = 0.5, precision_at_most_7d = 1.0, precision_less_or_equal_3 = 0.5, recall_3_7 = 0.0, recall_7_14 = nan, recall_above_14 = nan, recall_at_1 = 0.5, recall_at_2 = 1.0, recall_at_most_7d = 1.0, recall_less_or_equal_3 = 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: gs://cluster-data/demo/data/output/model/model.ckpt-500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0125 01:40:01.072910 140371859998464 tf_logging.py:115] Saving 'checkpoint_path' summary for global step 500: gs://cluster-data/demo/data/output/model/model.ckpt-500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5,\n",
       " 'auc_pr_at_most_7d': 1.0,\n",
       " 'auc_roc_at_most_7d': 0.0,\n",
       " 'average_loss': 1.2013555,\n",
       " 'global_step': 500,\n",
       " 'loss': 1.2013555,\n",
       " 'precision_3_7': nan,\n",
       " 'precision_7_14': nan,\n",
       " 'precision_above_14': nan,\n",
       " 'precision_at_1': 0.5,\n",
       " 'precision_at_2': 0.5,\n",
       " 'precision_at_most_7d': 1.0,\n",
       " 'precision_less_or_equal_3': 0.5,\n",
       " 'recall_3_7': 0.0,\n",
       " 'recall_7_14': nan,\n",
       " 'recall_above_14': nan,\n",
       " 'recall_at_1': 0.5,\n",
       " 'recall_at_2': 1.0,\n",
       " 'recall_at_most_7d': 1.0,\n",
       " 'recall_less_or_equal_3': 1.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=validation_input_fn, steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  2019-01-25T01:36:44Z  gs://cluster-data/demo/data/output/model/\n",
      "       265  2019-01-25T01:36:47Z  gs://cluster-data/demo/data/output/model/checkpoint\n",
      "   9706057  2019-01-23T20:02:55Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548273732.cluster-002-m\n",
      "   9706130  2019-01-23T23:09:07Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548284902.cluster-002-m\n",
      "   9706133  2019-01-24T22:49:19Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548370118.cluster-002-m\n",
      "   9706133  2019-01-24T23:54:57Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548374056.cluster-001-m\n",
      "   9706133  2019-01-25T01:36:50Z  gs://cluster-data/demo/data/output/model/events.out.tfevents.1548380168.cluster-002-m\n",
      "   5790532  2019-01-25T01:36:17Z  gs://cluster-data/demo/data/output/model/graph.pbtxt\n",
      "         8  2019-01-23T23:08:39Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.data-00000-of-00002\n",
      "   5393904  2019-01-23T23:08:38Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.data-00001-of-00002\n",
      "      5424  2019-01-23T23:08:40Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.index\n",
      "   2603529  2019-01-23T23:08:43Z  gs://cluster-data/demo/data/output/model/model.ckpt-100.meta\n",
      "         8  2019-01-24T22:48:53Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.data-00000-of-00002\n",
      "   5393904  2019-01-24T22:48:52Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.data-00001-of-00002\n",
      "      5424  2019-01-24T22:48:53Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.index\n",
      "   2603529  2019-01-24T22:48:57Z  gs://cluster-data/demo/data/output/model/model.ckpt-200.meta\n",
      "         8  2019-01-24T23:54:31Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.data-00000-of-00002\n",
      "   5393904  2019-01-24T23:54:31Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.data-00001-of-00002\n",
      "      5424  2019-01-24T23:54:32Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.index\n",
      "   2603529  2019-01-24T23:54:35Z  gs://cluster-data/demo/data/output/model/model.ckpt-300.meta\n",
      "         8  2019-01-25T01:36:23Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.data-00000-of-00002\n",
      "   5393904  2019-01-25T01:36:22Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.data-00001-of-00002\n",
      "      5424  2019-01-25T01:36:23Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.index\n",
      "   2603529  2019-01-25T01:36:27Z  gs://cluster-data/demo/data/output/model/model.ckpt-400.meta\n",
      "         8  2019-01-25T01:36:45Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.data-00000-of-00002\n",
      "   5393904  2019-01-25T01:36:45Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.data-00001-of-00002\n",
      "      5424  2019-01-25T01:36:46Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.index\n",
      "   2603529  2019-01-25T01:36:50Z  gs://cluster-data/demo/data/output/model/model.ckpt-500.meta\n",
      "                                 gs://cluster-data/demo/data/output/model/eval/\n",
      "TOTAL: 28 objects, 94335708 bytes (89.97 MiB)\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gsutil ls -l ${MODEL_IN_GCS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Deploy ML Model to Cloud ML</h2>\n",
    "<ul>\n",
    "    <li>The trained ML Model will be deployed to CoudML for serving </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
