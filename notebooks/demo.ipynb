{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> HIMSS Demo - HealtheDatalab </h1>\n",
    "<h2> Structured Machine Learning using Tensorflow </h2>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Prerequisites - Installation of dependencies/project setup\n",
    "<li> Preparation of data (Bunsen library/HealtheDatalab library)\n",
    "<li> Label generation \n",
    "<li> Features generation (TF Sequence examples Generation)\n",
    "<li> Train and Evaluate Machine Learning Model\n",
    "<li> Demonstrate prediction (los) using the model\n",
    "</ol>\n",
    "<hr />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Prerequisites </h2>\n",
    "Install/upgrade packages\n",
    "Execute this only once.\n",
    "Reset kernel after this cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preparation of data </h2>\n",
    "This cell creates FHIR bundles from RAW Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Enable Hive support for our session so we can save resources as Hive tables\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "from bunsen.stu3.bundles import load_from_directory, extract_entry, write_to_database\n",
    "\n",
    "# Load and cache the bundles so we don't reload them every time.\n",
    "bundles = load_from_directory(spark, 'gs://bunsen/data/bundles').cache()\n",
    "\n",
    "# Get the encounters and patients\n",
    "encounters = extract_entry(spark, bundles, 'encounter')\n",
    "patients = extract_entry(spark, bundles, 'patient')\n",
    "\n",
    "# (TBD) The json bundles are transformed to TF record format\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Label generation </h2>\n",
    "Input: FHIR bundles\n",
    "Output: Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "import apache_beam as beam\n",
    "from proto.stu3 import google_extensions_pb2\n",
    "from proto.stu3 import resources_pb2\n",
    "from py.google.fhir.labels import encounter\n",
    "from py.google.fhir.labels import label\n",
    "\n",
    "@beam.typehints.with_input_types(resources_pb2.Bundle)\n",
    "@beam.typehints.with_output_types(google_extensions_pb2.EventLabel)\n",
    "class LengthOfStayRangeLabelAt24HoursFn(beam.DoFn):\n",
    "  \"\"\"Converts Bundle into length of stay range at 24 hours label.\n",
    "\n",
    "    Cohort: inpatient encounter that is longer than 24 hours\n",
    "    Trigger point: 24 hours after admission\n",
    "    Label: multi-label for length of stay ranges, see label.py for detail\n",
    "  \"\"\"\n",
    "\n",
    "  def process(self, bundle):\n",
    "    \"\"\"Iterate through bundle and yield label.\n",
    "\n",
    "    Args:\n",
    "      bundle: input stu3.Bundle proto\n",
    "    Yields:\n",
    "      stu3.EventLabel proto.\n",
    "    \"\"\"\n",
    "    patient = encounter.GetPatient(bundle)\n",
    "    if patient is not None:\n",
    "      # Cohort: inpatient encounter > 24 hours.\n",
    "      for enc in encounter.Inpatient24HrEncounters(bundle):\n",
    "        for one_label in label.LengthOfStayRangeAt24Hours(patient, enc):\n",
    "          yield one_label\n",
    "          \n",
    "          \n",
    "          \n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.metrics import Metrics\n",
    "from apache_beam.metrics.metric import MetricsFilter\n",
    "\n",
    "import apache_beam as beam\n",
    "import re\n",
    "\n",
    "\n",
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "google_cloud_options.project = 'grand-magpie-222719'\n",
    "google_cloud_options.job_name = 'job1'\n",
    "google_cloud_options.staging_location = 'gs://de-testbunsen/staging'\n",
    "google_cloud_options.temp_location = 'gs://de-testbunsen/temp'\n",
    "options.view_as(StandardOptions).runner = 'DirectRunner'\n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "\n",
    "bundles = p | 'read' >> beam.io.ReadFromTFRecord(\n",
    "    'gs://de-testbunsen/data/test_bundle.tfrecord-00000-of-00001', coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "    \n",
    "labels = bundles | 'BundleToLabel' >> beam.ParDo(\n",
    "    LengthOfStayRangeLabelAt24HoursFn())\n",
    "_ = labels | beam.io.WriteToTFRecord(\n",
    "    'gs://de-testbunsen/data',\n",
    "    coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel))\n",
    "\n",
    "\n",
    "p.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Features generation </h2>\n",
    "Input: FHIR bundles, GS Bucket with \"Bundle (tfrecord) and GS Bucket \"Labels (tfrecord)\"\n",
    "Output: tf seqex example. This has 1. Context - patient information 2. Timeseries data - such as encounters.\n",
    "Status: direct runner - time consuming for 1000+ records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Train and Evaluate Machine Learning Model </h2>\n",
    "Input: Training and Evaluation Dataset\n",
    "Output: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Progress: Basic building blocks WORK \n",
    "#Need to stitch up ouptut of the previous cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Demonstrate prediction (los) using the model </h2>\n",
    "Input: New Data set\n",
    "Output: Length Of Stay Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use 3rd set of unseen data by model\n",
    "#Use trained model from previous cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINALLY - WHEN YOU ARE READY - deploy to CMLE - BOOM!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
