{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> HIMSS Demo - HealtheDatalab </h1>\n",
    "\n",
    "<h2> Structured Machine Learning using Tensorflow </h2>\n",
    "<hr />\n",
    "This notebook demonstrates a process to train, evaluate and deploy a ML model to CloudML. It leverages a pre-built machine learning model to predict Length of Stay in ED and inpatient care settings\n",
    "<h3>\n",
    "<br />\n",
    "<ol>\n",
    "<li> Access, Analize & Visualize Data using HealtheDataLab </li> <br />\n",
    "<li> Label generation - Generate Labels in TFRecord format </li> <br />\n",
    "<li> Generate TFSequenceExamples with context = patient + time series data = encounters </li> <br />\n",
    "<li> Train and Evaluate Machine Learning Model </li> <br />\n",
    "<li> Deploy ML Model to CloudML </li> <br />\n",
    "</ol>\n",
    "</h3>\n",
    "<hr />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test code \n"
     ]
    }
   ],
   "source": [
    "from dummy import foo\n",
    "foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 280\n",
      "drwxr-xr-x 6 root root  4096 Jan 21 19:49 .\n",
      "drwxr-xr-x 6 root root  4096 Jan 15 09:54 ..\n",
      "-rw-r--r-- 1 root root 21308 Jan 20 03:53 AnalyzeVisits.ipynb\n",
      "-rw-r--r-- 1 root root 19866 Jan 15 10:26 bundles_to_labels_dr.ipynb\n",
      "-rw-r--r-- 1 root root  5577 Jan 15 17:17 bundles_to_seqex_dr.ipynb\n",
      "-rw-r--r-- 1 root root 95166 Jan 15 11:16 bunsen_de_tutorial.ipynb\n",
      "-rw-r--r-- 1 root root 53071 Jan 20 03:55 bunsen_getting_started.ipynb\n",
      "-rw-r--r-- 1 root root  7544 Jan 15 09:54 demo.ipynb\n",
      "drwxr-xr-x 7 root root  4096 Jan 21 19:43 .git\n",
      "-rw-r--r-- 1 root root 12748 Jan 21 19:48 hdl_demo.ipynb\n",
      "drwxr-xr-x 2 root root  4096 Jan 20 03:53 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 root root 28820 Jan 17 02:56 linear-model-notebook.ipynb\n",
      "drwxr-xr-x 2 root root  4096 Jan 15 09:54 misc\n",
      "drwxr-xr-x 2 root root  4096 Jan 20 04:16 __pycache__\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Access, Analize & Visualize Data using HealtheDataLab </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, date\n",
    "from bunsen.stu3.bundles import load_from_directory, extract_entry\n",
    "\n",
    "# Enable Hive support for our session so we can save resources as Hive tables\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# Load and cache the bundles so we don't reload them every time.\n",
    "bundles = load_from_directory(spark, 'gs://cluster-data/demo/data/synthea/fhir/').cache()\n",
    "\n",
    "# Get patients and encounters data from bundles\n",
    "patients = extract_entry(spark, bundles, 'patient')\n",
    "encounters = extract_entry(spark, bundles, 'encounter')\n",
    "\n",
    "def age(birthdate):\n",
    "    born = datetime.strptime(birthdate, \"%Y-%m-%d\")\n",
    "    today = date.today()\n",
    "    return str(today.year - born.year - ((today.month, today.day) < (born.month, born.day)))\n",
    "\n",
    "pats = patients.select('id','gender', 'birthDate', 'address.city', 'address.state', 'address.country') \n",
    "\n",
    "#pats['birthDate'] = pats['birthDate'].apply(age)\n",
    "patsDF = pats.limit(10).toPandas()\n",
    "patsDF['age'] = patsDF['birthDate'].apply(age)\n",
    "display(patsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def los(row):\n",
    "    #start = datetime.strptime(row['start'], '%Y-%m-%dT%H:%M:%S%z')\n",
    "    start = parse(row['start'])\n",
    "    end = parse(row['end'])\n",
    "    #end = datetime.strptime(row['end'], '%Y-%m-%dT%H:%M:%S%z')\n",
    "    los = end-start\n",
    "    return str(los)\n",
    "  \n",
    "encs=encounters.select('subject.reference', \n",
    "                  'class.code', \n",
    "                  'period.start', \n",
    "                  'period.end') \\\n",
    "          .where(col('class.code').isin(\"inpatient\", \"emergency\"))\n",
    "\n",
    "\n",
    "encsDF = encs.limit(10).toPandas()\n",
    "encsDF['los'] = encsDF.apply(los, axis=1)\n",
    "display(encsDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE(WIP)\n",
    "PROJECT = 'dp-workspace'\n",
    "REGION = 'us-west1'\n",
    "BUCKET = 'cluster-data'\n",
    "\n",
    "import os\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Preparation of data - Create input data bundles in TFRecord format</h2>\n",
    "This cell creates FHIR bundles from RAW Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Enable Hive support for our session so we can save resources as Hive tables\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "from bunsen.stu3.bundles import load_from_directory, extract_entry, write_to_database\n",
    "\n",
    "# Load and cache the raw data (FHIR bundles) from Google Cloud Storage bucket so we don't reload them every time.\n",
    "bundles = load_from_directory(spark, 'gs://bunsen/data/bundles').cache()\n",
    "\n",
    "# Create TFRecords from the raw FHIR bundles (one line to create TFrecordas)\n",
    "#TODO ........\n",
    "#For now we have generated a sample TF Record and stored in a following cloud storage bucket: gs://cluster-data/demo/data/test_bundle.tfrecord-00000-of-00001 \n",
    "#Text version of the test_bundle.tfrecord-00000-of-00001 is in file: bundle_1.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls -l gs://${BUCKET}/demo/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Label generation - Generate Labels in TFRecord format</h2>\n",
    "Input: FHIR bundles\n",
    "Output: Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app\n",
    "from absl import flags\n",
    "import apache_beam as beam\n",
    "from proto.stu3 import google_extensions_pb2\n",
    "from proto.stu3 import resources_pb2\n",
    "from py.google.fhir.labels import encounter\n",
    "from py.google.fhir.labels import label\n",
    "\n",
    "@beam.typehints.with_input_types(resources_pb2.Bundle)\n",
    "@beam.typehints.with_output_types(google_extensions_pb2.EventLabel)\n",
    "class LengthOfStayRangeLabelAt24HoursFn(beam.DoFn):\n",
    "  \"\"\"Converts Bundle into length of stay range at 24 hours label.\n",
    "\n",
    "    Cohort: inpatient encounter that is longer than 24 hours\n",
    "    Trigger point: 24 hours after admission\n",
    "    Label: multi-label for length of stay ranges, see label.py for detail\n",
    "  \"\"\"\n",
    "\n",
    "  def process(self, bundle):\n",
    "    \"\"\"Iterate through bundle and yield label.\n",
    "\n",
    "    Args:\n",
    "      bundle: input stu3.Bundle proto\n",
    "    Yields:\n",
    "      stu3.EventLabel proto.\n",
    "    \"\"\"\n",
    "    patient = encounter.GetPatient(bundle)\n",
    "    if patient is not None:\n",
    "      # Cohort: inpatient encounter > 24 hours.\n",
    "      for enc in encounter.Inpatient24HrEncounters(bundle):\n",
    "        for one_label in label.LengthOfStayRangeAt24Hours(patient, enc):\n",
    "          yield one_label\n",
    "          \n",
    "          \n",
    "          \n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.metrics import Metrics\n",
    "from apache_beam.metrics.metric import MetricsFilter\n",
    "\n",
    "import apache_beam as beam\n",
    "import re\n",
    "\n",
    "\n",
    "options = PipelineOptions()\n",
    "google_cloud_options = options.view_as(GoogleCloudOptions)\n",
    "#google_cloud_options.project = 'dp-workspace'\n",
    "google_cloud_options.project = PROJECT\n",
    "google_cloud_options.job_name = 'job1'\n",
    "google_cloud_options.staging_location = 'gs://cluster-data/demo/staging'\n",
    "google_cloud_options.temp_location = 'gs://cluster-data/demo/temp'\n",
    "options.view_as(StandardOptions).runner = 'DirectRunner'\n",
    "\n",
    "p = beam.Pipeline(options=options)\n",
    "input_bundle = 'gs://cluster-data/demo/data/test_bundle.tfrecord-00000-of-00001'\n",
    "output_file_prefix = 'gs://cluster-data/demo/data/output/label'\n",
    "\n",
    "bundles = p | 'read' >> beam.io.ReadFromTFRecord(input_bundle, coder=beam.coders.ProtoCoder(resources_pb2.Bundle))\n",
    "    \n",
    "labels = bundles | 'BundleToLabel' >> beam.ParDo(\n",
    "    LengthOfStayRangeLabelAt24HoursFn())\n",
    "_ = labels | beam.io.WriteToTFRecord(output_file_prefix,\n",
    "    coder=beam.coders.ProtoCoder(google_extensions_pb2.EventLabel))\n",
    "\n",
    "\n",
    "p.run().wait_until_finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above cell generates a label TFRecord and stores it into a GS Bucket\n",
    "%bash\n",
    "gsutil ls -l gs://cluster-data/demo/data/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Generate TFSequenceExamples with context = patient + time series data = encounters</h2>\n",
    "Input: FHIR bundles\n",
    "Output: Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Train and Evaluate Machine Learning Model </h2>\n",
    "Input: Training and Evaluation Dataset\n",
    "Output: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE(WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 6. Deploy ML Model to ??? </h2>\n",
    "Input: New Data set\n",
    "Output: Length Of Stay Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE(WIP)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
