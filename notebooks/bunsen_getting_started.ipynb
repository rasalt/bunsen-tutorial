{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook loads a collection of synthetic FHIR bundles and value sets and shows some simple queries. Running this first will set up the environment for other notebooks in the tutorial\n",
    "\n",
    "## Setup Tasks\n",
    "Some setup before the real show begins..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "GCS_BUCKET = 'gs://cluster12-bkt'\n",
    "FHIR_BUNDLES = GCS_BUCKET+'/synthea/fhir'\n",
    "\n",
    "# Enable Hive support for our session so we can save resources as Hive tables\n",
    "spark = SparkSession.builder \\\n",
    "                    .config('hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Synthetic Data\n",
    "This tutorial uses data generated by Synthea. It is simply a directory of STU3 bundles visible included in the tutorial; you can see it in the bundles directory.\n",
    "\n",
    "Let's load the bundles and examine a couple data types in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunsen.stu3.bundles import load_from_directory, extract_entry, write_to_database\n",
    "\n",
    "# Load and cache the bundles so we don't reload them every time.\n",
    "bundles = load_from_directory(spark, FHIR_BUNDLES).cache()\n",
    "\n",
    "# Get the observation and encounters\n",
    "observations = extract_entry(spark, bundles, 'observation')\n",
    "encounters = extract_entry(spark, bundles, 'encounter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bunsen documentation\n",
    "To get help using functions like *load_from_directory* or *extract_entry*, you can see the documentation at https://engineering.cerner.com/bunsen or via Python's help system, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(extract_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated from FHIR Resource Definitions\n",
    "The Apache Spark datasets used here are fully generated from the FHIR resource definitions, with every field mapped one-to-one. For instance, here is the fully Spark schema of the Observation resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some data\n",
    "The next step will load some data and inspect it. Since Spark lazily delays execution until output is needed, all of the work will be done here. This can take several seconds or longer depending on the machine, but users can check its status by looking at the [Spark application page](http://localhost:4040).\n",
    "\n",
    "For now, let's just turn our encounter resources into a simple table of all encounters since 2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "encounters.select('subject.reference', \n",
    "                  'class.code', \n",
    "                  'period.start', \n",
    "                  'period.end') \\\n",
    "          .where(col('start') > '2013') \\\n",
    "          .limit(10) \\\n",
    "          .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploding nested lists\n",
    "FHIR's nested structures group related data, making many workloads simpler. We can reference such nested structures directly, and \"explode\" nested lists when needed to analyze them. Let's build a table of all observation codes in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "codes = observations.select('subject',\n",
    "                            explode('code.coding').alias('coding')) \\\n",
    "                    .select('subject.reference', \n",
    "                            'coding.system', \n",
    "                            'coding.code',\n",
    "                            'coding.display')\n",
    "                    \n",
    "codes.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing data\n",
    "Our datasets become much easier to analyze once they've been projected onto a simpler model that suits the proble at hand. The code below simply shows the most frequent observation codes in our synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes.groupBy('system', 'code', 'display') \\\n",
    "     .count() \\\n",
    "     .orderBy('count', ascending=False) \\\n",
    "     .limit(10) \\\n",
    "     .toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing resources to a database\n",
    "Directly loading JSON or XML FHIR bundles is useful for ingesting and early exploration of data, but a more efficient format works better repeated use. Since Bunsen encodes resources natively in Apache Spark dataframes, we can take advantage of Spark's ability to write it to a Hive database. Bunsen offers the *write_to_database* function as a convenient way to write resources from bundles to a database, with a table for each resource. \n",
    "\n",
    "Note that each table preserves the original, nested structure definition of the FHIR resource, and is field-for-field equivalent. \n",
    "\n",
    "The cell below will save our test data to tables in the \"tutorial_small\" database. When running it, you can see progress in the Spark UI at http://localhost:4040.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = ['allergyintolerance',\n",
    "             'careplan',\n",
    "             'claim',\n",
    "             'condition',\n",
    "             'encounter',\n",
    "             'immunization',\n",
    "             'medication',\n",
    "             'medicationrequest',\n",
    "             'observation',\n",
    "             'organization',\n",
    "             'patient',\n",
    "             'procedure']\n",
    "\n",
    "write_to_database(spark, \n",
    "                  bundles, \n",
    "                  'tutorial_small',\n",
    "                  resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a Hive database\n",
    "Now that we've saved our data to a Hive database, we can easily view and query the tables with Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql('use tutorial_small')\n",
    "spark.sql('show tables').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select subject.reference, \n",
    "       count(*) cnt\n",
    "from encounter\n",
    "where class.code != 'WELLNESS' and\n",
    "      period.start > '2013'\n",
    "group by subject.reference\n",
    "order by cnt desc\n",
    "limit 10\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Valuesets\n",
    "Bunsen has built-in support for working with FHIR valuesets. As a convenience, the APIs in the bunsen.stu3.codes package offers ways to save valuesets to Hive tables that are more easily used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunsen.stu3.codes import create_value_sets\n",
    "\n",
    "# Load the valuesets from bundles\n",
    "valueset_bundles = load_from_directory(spark, 'gs://bunsen/data/valuesets')\n",
    "valueset_data = extract_entry(spark, valueset_bundles, 'valueset')\n",
    "\n",
    "# Import the value sets and save them to an ontologies database for easy future use\n",
    "spark.sql('create database tutorial_ontologies')\n",
    "\n",
    "create_value_sets(spark).with_value_sets(valueset_data) \\\n",
    "                        .write_to_database('tutorial_ontologies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a valuesets table, which uses the FHIR ValueSet schema, that we can easily explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('tutorial_ontologies.valuesets').select('url', 'version', 'description').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also creates a \"values\" table that we can more easily look at the values in our valuesets. This makes valuesets that may contain many thousands of values easier to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table('tutorial_ontologies.values').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Valuesets in Queries\n",
    "Finally, we illustrate how we can easily use FHIR valuesets within Spark SQL. Bunsen provides an *in_valueset* user-defined function that can be invoked directly from SQL, so users can easily work with valuesets without needing complex joins to separate ontology tables.\n",
    "\n",
    "First, we will push some interesting valuesets to the cluster with the *push_valuesets* function seen below. This uses Apache Spark's broadcast variables to get this reference data on each node, so it can be easily used. Details are in that function documentation, but typically users work with valuesets in one of three ways:\n",
    "\n",
    "* From a FHIR ValueSet resource, as illustrated here\n",
    "* As a collection of values in a Python structure\n",
    "* As an is-a relationship in some ontology, like LOINC or SNOMED.\n",
    "\n",
    "Further documentation can be viewed in the function documentation or via help(push_valuesets).\n",
    "\n",
    "Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bunsen.stu3.valuesets import push_valuesets, valueset\n",
    "\n",
    "# Push multiple valuesets for this example, even though we use only one.\n",
    "push_valuesets(spark, \n",
    "               {'ldl'               : [('http://loinc.org', '18262-6')],                \n",
    "                'hdl'               : [('http://loinc.org', '2085-9')],\n",
    "                'cholesterol'       : valueset('http://hl7.org/fhir/ValueSet/example-extensional', '20150622')},\n",
    "               database='tutorial_ontologies'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the above valuesets have been broadcast across our processing cluster, we can easily query them with the *in_valueset* user-defined function inline with our SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select subject.reference, \n",
    "       valueQuantity.value,\n",
    "       valueQuantity.unit\n",
    "from tutorial_small.observation\n",
    "where in_valueset(code, 'cholesterol')\n",
    "limit 10\n",
    "\"\"\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
